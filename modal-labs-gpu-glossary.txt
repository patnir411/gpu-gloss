Directory structure:
└── modal-labs-gpu-glossary/
    ├── README.md
    ├── LICENSE
    ├── package.json
    └── gpu-glossary/
        ├── readme.md
        ├── contributors.md
        ├── device-hardware.md
        ├── device-software.md
        ├── host-software.md
        ├── LICENSE
        ├── device-hardware/
        │   ├── core.md
        │   ├── cuda-core.md
        │   ├── cuda-device-architecture.md
        │   ├── gpu-ram.md
        │   ├── graphics-processing-cluster.md
        │   ├── l1-data-cache.md
        │   ├── load-store-unit.md
        │   ├── register-file.md
        │   ├── special-function-unit.md
        │   ├── streaming-multiprocessor-architecture.md
        │   ├── streaming-multiprocessor.md
        │   ├── tensor-core.md
        │   ├── texture-processing-cluster.md
        │   └── warp-scheduler.md
        ├── device-software/
        │   ├── compute-capability.md
        │   ├── cooperative-thread-array.md
        │   ├── cuda-programming-model.md
        │   ├── global-memory.md
        │   ├── kernel.md
        │   ├── memory-hierarchy.md
        │   ├── parallel-thread-execution.md
        │   ├── registers.md
        │   ├── shared-memory.md
        │   ├── streaming-assembler.md
        │   ├── thread-block-grid.md
        │   ├── thread-block.md
        │   ├── thread.md
        │   └── warp.md
        └── host-software/
            ├── cuda-binary-utilities.md
            ├── cuda-c.md
            ├── cuda-driver-api.md
            ├── cuda-runtime-api.md
            ├── cuda-software-platform.md
            ├── cupti.md
            ├── libcuda.md
            ├── libcudart.md
            ├── libnvml.md
            ├── nsight-systems.md
            ├── nvcc.md
            ├── nvidia-gpu-drivers.md
            ├── nvidia-ko.md
            ├── nvidia-smi.md
            ├── nvml.md
            └── nvrtc.md

================================================
FILE: README.md
================================================
# GPU Glossary

This repository contains the source content for the
[Modal GPU Glossary](https://modal.com/gpu-glossary),
a dictionary of terms related to programming GPUs,
with a focus on the GPUs that run on the [Modal platform](https://modal.com),
NVIDIA GPUs.

## Licenses

[![MIT License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![CC BY 4.0](https://licensebuttons.net/l/by/4.0/80x15.png)](gpu-glossary/LICENSE)


All files in the `gpu-glossary` folder of this repository are licensed under the 
[Creative Commons Attribution 4.0 International (CC BY 4.0) License](https://creativecommons.org/licenses/by/4.0/).
See [`gpu-glossary/LICENSE`](gpu-glossary/LICENSE) for details.

The remainder of the files in this repository are licensed under the
[MIT License](https://opensource.org/license/mit).
See [`LICENSE`](LICENSE) for details.




================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2024 - 2025 Modal Labs, Inc.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: package.json
================================================
{
  "name": "gpu-glossary",
  "private": true,
  "scripts": {
    "format": "prettier --write --ignore-unknown gpu-glossary/*",
    "format:check": "prettier --check --ignore-unknown gpu-glossary/*"
  },
  "devDependencies": {
    "prettier": "^3.5.3"
  },
  "prettier": {
    "proseWrap": "always",
    "trailingComma": "all"
  }
}



================================================
FILE: gpu-glossary/readme.md
================================================
---
title: README
---

<pre class="text-xs md:text-base font-mono whitespace-pre">
 ██████╗ ██████╗ ██╗   ██╗
██╔════╝ ██╔══██╗██║   ██║
██║  ███╗██████╔╝██║   ██║
██║   ██║██╔═══╝ ██║   ██║
╚██████╔╝██║     ╚██████╔╝
 ╚═════╝ ╚═╝      ╚═════╝
 ██████╗ ██╗      ██████╗ ███████╗███████╗ █████╗ ██████╗ ██╗   ██╗
██╔════╝ ██║     ██╔═══██╗██╔════╝██╔════╝██╔══██╗██╔══██╗╚██╗ ██╔╝
██║  ███╗██║     ██║   ██║███████╗███████╗███████║██████╔╝ ╚████╔╝
██║   ██║██║     ██║   ██║╚════██║╚════██║██╔══██║██╔══██╗  ╚██╔╝
╚██████╔╝███████╗╚██████╔╝███████║███████║██║  ██║██║  ██║   ██║
 ╚═════╝ ╚══════╝ ╚═════╝ ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝  ╚═╝   ╚═╝
 </pre>

We wrote this glossary to solve a problem we ran into working with GPUs here at
[Modal](/): the documentation is fragmented, making it difficult to connect
concepts at different levels of the stack, like
[Streaming Multiprocessor Architecture](/gpu-glossary/device-hardware/streaming-multiprocessor-architecture),
[Compute Capability](/gpu-glossary/device-software/compute-capability), and
[nvcc compiler flags](/gpu-glossary/host-software).

So we've read the
[PDFs from NVIDIA](https://docs.nvidia.com/cuda/pdf/PTX_Writers_Guide_To_Interoperability.pdf),
lurked in the [good Discords](https://discord.gg/gpumode), and even bought
[dead-tree textbooks](https://www.amazon.com/Professional-CUDA-Programming-John-Cheng/dp/1118739329)
to put together a glossary that spans the whole stack in one place.

This glossary, unlike a PDF or a Discord or a book, is a _hypertext document_ --
all pages are inter-linked with one another, so you can jump down to read about
the [Warp Scheduler](/gpu-glossary/device-hardware/warp-scheduler) so you can
better understand the [threads](/gpu-glossary/device-software/thread) that you
came across in the article on the
[CUDA programming model](/gpu-glossary/host-software/cuda-c).

You can also read it linearly. To navigate between pages, use the arrow keys,
the arrows at the bottom of each page, or the table of contents (in the sidebar
on desktop or in the hamburger menu on mobile).

The source for the glossary is available
[on GitHub](https://github.com/modal-labs/gpu-glossary).



================================================
FILE: gpu-glossary/contributors.md
================================================
---
title: Contributors
---

This list is incomplete; you can help by
[expanding it](https://github.com/modal-labs/gpu-glossary).

### Authors

- [Charles Frye](https://twitter.com/charles_irl) wrote the majority of the
  material and takes full responsibility for any errors.
- [Matthew Nappo](https://www.linkedin.com/in/mattnappo/) wrote the initial
  internal "GPU Glossary" document from which this sprung.
- [You](mailto:glossary@modal.com?subject=Contributing%20to%20GPU%20glossary)
  can contribute to keep the glossary up-to-date and erratum-free!

### Design

- [Sona Dolasia](https://twitter.com/teenychairs) designed the glossary.
- [Anna Carey](https://twitter.com/anna_carey) implemented the design and UX.

### Review

- [Abhinav Upadhyay](https://twitter.com/abhi9u) of
  [Coding Confessions](https://blog.codingconfessions.com/) and
  [`@Pauleonix`](https://github.com/pauleonix) of the
  [GPU MODE Discord](https://discord.gg/gpumode), from outside Modal, provided
  valuable external technical review of the glossary. We particularly thank
  Abhinav for his perspective on comparisons with CPUs and Pauleonix for his
  detailed insights on GPU hardware internals.
- [Akshat Bubna](https://twitter.com/akshat_b),
  [Nathan Wang](https://www.linkedin.com/in/nathan-r-wang/), and
  [Colin Weld](https://www.linkedin.com/in/colin-weld/) gave technical feedback
  on early drafts of the glossary.
- [Eric Zhang](https://twitter.com/ekzhang1) and
  [Ro Arepally](https://twitter.com/rarepally) reviewed the design and
  implementation.

### Acknowledgements

- [Mark Saroufim](https://twitter.com/marksaroufim) and Andreas Kopf for
  bringing together the [GPU MODE Discord community](https://discord.gg/gpumode)
- [Fabien Sanglard](https://twitter.com/fabynou) for authoring an
  [excellent history of CUDA GPUs](https://fabiensanglard.net/cuda)
- Jen-Hsun Huang for leading an organization that makes some pretty decent chips

### Error Correction

We thank the following GPU enthusiasts who came in through the world wide web to
correct errors:

<!-- This list is ordered alphabetically by the anchor text, ignoring case -->

- [Alex Zhang](https://alexzhang13.github.io/blog/2024/efficient-dl/)
- [Erik Schultheis](https://www.linkedin.com/in/erik-schultheis-606a52119/)
- Ismail Zaidi
- [Michal Nawrot](https://github.com/michalnawrot)
- [Nicolas Blin](https://www.nicolas-blin.fr/)



================================================
FILE: gpu-glossary/device-hardware.md
================================================
---
title: Device Hardware
---

These terms and technologies are physical components of the GPU — the "device"
in NVIDIA's lingo.



================================================
FILE: gpu-glossary/device-software.md
================================================
---
title: Device Software
---

These terms and technologies are used for software that runs on GPU — the
"device" in NVIDIA's lingo.



================================================
FILE: gpu-glossary/host-software.md
================================================
---
title: Host Software
---

These terms and technologies are used on the CPU (the "host" in NVIDIA's lingo)
when running GPU programs.



================================================
FILE: gpu-glossary/LICENSE
================================================
Creative Commons Attribution 4.0 International (CC BY 4.0)

This folder (`gpu-glossary/`) contains Markdown files licensed under the 
Creative Commons Attribution 4.0 International (CC BY 4.0) license. 
You are free to share and adapt them as long as you provide attribution.

Full license: https://creativecommons.org/licenses/by/4.0/legalcode



================================================
FILE: gpu-glossary/device-hardware/core.md
================================================
---
title: What is a GPU Core?
---

The cores are the primary compute units that make up the
[Streaming Multiprocessors (SMs)](/gpu-glossary/device-hardware/streaming-multiprocessor).

![The internal architecture of an H100 GPU's Streaming Multiprocessors. CUDA and Tensor Cores are shown in green. Modified from NVIDIA's [H100 white paper](https://resources.nvidia.com/en-us-tensor-core).](themed-image://gh100-sm.svg)

Examples of GPU core types include
[CUDA Cores](/gpu-glossary/device-hardware/cuda-core) and
[Tensor Cores](/gpu-glossary/device-hardware/tensor-core).

Though GPU cores are comparable to CPU cores in that they are the component that
effects actual computations, this analogy can be quite misleading. Instead, it
is perhaps more helpful to take the viewpoint of the
[quantitative computer architect](https://archive.org/details/computerarchitectureaquantitativeapproach6thedition)
and think of them as "pipes" into which data goes in and out of which
transformed data is returned. These pipes are associated in turn with specific
[instructions](/gpu-glossary/device-software/streaming-assembler) from the
hardware's perspective and with different fundamental affordances of throughput
from the programmers' (e.g. floating point matrix multiplication arithmetic
throughput in the case of the
[Tensor Cores](/gpu-glossary/device-hardware/tensor-core)).

The [SMs](/gpu-glossary/device-hardware/streaming-multiprocessor) are closer to
being the equivalent of CPU cores, in that they have
[register memory](/gpu-glossary/device-hardware/register-file) to store
information, cores to transform it, and an
[instruction scheduler](/gpu-glossary/device-hardware/warp-scheduler) to specify
and command transformations.



================================================
FILE: gpu-glossary/device-hardware/cuda-core.md
================================================
---
title: What is a CUDA Core?
---

The CUDA Cores are GPU [cores](/gpu-glossary/device-hardware/core) that execute
scalar arithmetic instructions.

![The internal architecture of an H100 SM. The CUDA Cores and Tensor Cores are depicted in green. Note the larger size and lower number of Tensor Cores. Modified from NVIDIA's [H100 white paper](https://resources.nvidia.com/en-us-tensor-core).](themed-image://gh100-sm.svg)

They are to be contrasted with the
[Tensor Cores](/gpu-glossary/device-hardware/tensor-core), which execute matrix
operations.

Unlike CPU cores, instructions issued to CUDA Cores are not generally
independently scheduled. Instead, groups of cores are issued the same
instruction simultaneously by the
[Warp Scheduler](/gpu-glossary/device-hardware/warp-scheduler) but apply it to
different [registers](/gpu-glossary/device-software/registers). Commonly, these
groups are of size 32, the size of a [warp](/gpu-glossary/device-software/warp),
but for contemporary GPUs groups can contain as little as one thread, at a cost
to performance.

The term "CUDA Core" is slightly slippery: in different
[Streaming Multiprocessor architectures](/gpu-glossary/device-hardware/streaming-multiprocessor-architecture)
CUDA Cores can consist of different units -- a different mixture of 32 bit
integer and 32 bit and 64 bit floating point units.

So, for example, the
[H100 whitepaper](https://resources.nvidia.com/en-us-tensor-core) indicates that
an H100 GPU's
[Streaming Multiprocessors (SMs)](/gpu-glossary/device-hardware/streaming-multiprocessor)
each have 128 "FP32 CUDA Cores", which accurately counts the number of 32 bit
floating point units but is double the number of 32 bit integer or 64 bit
floating point units (as evidenced by the diagram above). For estimating
performance, it's best to look directly at the number of hardware units for a
given operation.



================================================
FILE: gpu-glossary/device-hardware/cuda-device-architecture.md
================================================
---
title: What is a CUDA Device Architecture?
---

CUDA stands for _Compute Unified Device Architecture_. Depending on the context,
"CUDA" can refer to multiple distinct things: a high-level device architecture,
a
[parallel programming model](/gpu-glossary/device-software/cuda-programming-model)
for architectures with that design, or a
[software platform](/gpu-glossary/host-software/cuda-software-platform) that
extends high-level languages like C to add that programming model.

The vision for CUDA is laid out in the
[Lindholm et al., 2008](https://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/lindholm08_tesla.pdf)
white paper. We highly recommend this paper, which is the original source for
many claims, diagrams, and even specific turns of phrase in NVIDIA's
documentation.

Here, we focus on the _device architecture_ part of CUDA. The core feature of a
"compute unified device architecture" is simplicity, relative to preceding GPU
architectures.

Prior to the GeForce 8800 and the Tesla data center GPUs it spawned, NVIDIA GPUs
were designed with a complex pipeline shader architecture that mapped software
shader stages onto heterogeneous, specialized hardware units. This architecture
was challenging for the software and hardware sides alike: it required software
engineers to map programs onto a fixed pipeline and forced hardware engineers to
guess the load ratios between pipeline steps.

![A diagram of a fixed-pipeline device architecture (G71). Note the presence of a separate group of processors for handling fragment and vertex shading. Adapted from [Fabien Sanglard's blog](https://fabiensanglard.net/cuda/).](themed-image://fixed-pipeline-g71.svg)

GPU devices with a unified architecture are much simpler: the hardware units are
entirely uniform, each capable of a wide array of computations. These units are
known as
[Streaming Multiprocessors (SMs)](/gpu-glossary/device-hardware/streaming-multiprocessor)
and their main subcomponents are the
[CUDA Cores](/gpu-glossary/device-hardware/cuda-core) and (for recent GPUs)
[Tensor Cores](/gpu-glossary/device-hardware/tensor-core).

![A diagram of a compute unified device architecture (G80). Note the absence of distinct processor types — all meaningful computation occurs in the identical [Streaming Multiprocessors](/gpu-glossary/device-hardware/streaming-multiprocessor) in the center of the diagram, fed with instructions for vertex, geometry, and pixel threads. Modified from [Peter Glazkowsky's 2009 white paper on the Fermi Architecture](https://www.nvidia.com/content/pdf/fermi_white_papers/p.glaskowsky_nvidia%27s_fermi-the_first_complete_gpu_architecture.pdf).](themed-image://cuda-g80.svg)

For an accessible introduction to the history and design of CUDA hardware
architectures, see [this blog post](https://fabiensanglard.net/cuda/) by Fabien
Sanglard. That blog post cites its (high-quality) sources, like NVIDIA's
[Fermi Compute Architecture white paper](https://fabiensanglard.net/cuda/Fermi_Compute_Architecture_Whitepaper.pdf).
The white paper by
[Lindholm et al. in 2008](https://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/lindholm08_tesla.pdf)
introducing the Tesla architecture is both well-written and thorough. The
[NVIDIA whitepaper for the Tesla P100](https://images.nvidia.com/content/pdf/tesla/whitepaper/pascal-architecture-whitepaper.pdf)
is less scholarly but documents the introduction of a number of features that
are critical for today's large-scale neural network workloads, like NVLink and
[on-package high-bandwidth memory](/gpu-glossary/device-hardware/gpu-ram).



================================================
FILE: gpu-glossary/device-hardware/gpu-ram.md
================================================
---
title: What is GPU RAM?
---

![In state-of-the-art GPUs like the H100, RAM is located on a die directly next to the processor's. Adapted from the Wikipedia page for [high-bandwidth memory](https://en.wikipedia.org/wiki/High_Bandwidth_Memory).](themed-image://hbm-schematic.svg)

The global memory of the GPU is a large (many megabytes to gigabytes) memory
store that is addressable by all of the GPU's
[Streaming Multiprocessors (SMs)](/gpu-glossary/device-hardware/streaming-multiprocessor).

It is also known as GPU RAM (random access memory) or video RAM (VRAM). It uses
Dynamic RAM (DRAM) cells, which are slower but smaller than the Static RAM
(SRAM) used in registers and shared memory. For details on DRAM and SRAM, we
recommend Ulrich Drepper's 2007 article
["What Every Programmer Should Know About Memory"](https://people.freebsd.org/~lstewart/articles/cpumemory.pdf).

It is generally not on the same die as the
[SMs](/gpu-glossary/device-hardware/streaming-multiprocessor), though in the
latest data center-grade GPUs like the H100, it is located on a shared
[interposer](https://en.wikipedia.org/wiki/Interposer) for decreased latency and
increased bandwidth (aka
"[high-bandwidth memory](https://en.wikipedia.org/wiki/High_Bandwidth_Memory)").

RAM is used to implement the
[global memory](/gpu-glossary/device-software/global-memory) of the
[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model)
and to store [register](/gpu-glossary/device-software/registers) data that
spills from the [register file](/gpu-glossary/device-hardware/register-file).

An H100 can store 80 GiB (687,194,767,360 bits) in its RAM.



================================================
FILE: gpu-glossary/device-hardware/graphics-processing-cluster.md
================================================
---
title: What is a Graphics/GPU Processing Cluster?
abbreviation: GPC
---

A GPC is a collection of
[Texture Processing Clusters (TPCs)](/gpu-glossary/device-hardware/texture-processing-cluster)
(themselves groups of
[Streaming Multiprocessors](/gpu-glossary/device-hardware/streaming-multiprocessor)
or SMs) plus a raster engine. Apparently, some people use NVIDIA GPUs for
graphics, for which the raster engine is important. Relatedly, the name used to
stand for Graphics Processing Cluster, but is now, e.g. in the
[NVIDIA CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html),
expanded as "GPU Processing Cluster".

For the latest
[compute capability](/gpu-glossary/device-software/compute-capability) 9.0 GPUs
like H100s, there is an additional layer of the
[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model)'s
thread hierarchy, a "cluster" of
[thread blocks](/gpu-glossary/device-software/thread-block), that are scheduled
onto the same GPC, just as the threads of a
[thread block](/gpu-glossary/device-software/thread-block) are scheduled onto
the same [SM](/gpu-glossary/device-hardware/streaming-multiprocessor), and have
their own level of the
[memory hierarchy](/gpu-glossary/device-software/memory-hierarchy). Elsewhere,
we elide discussion of this feature.



================================================
FILE: gpu-glossary/device-hardware/l1-data-cache.md
================================================
---
title: What is the L1 Data Cache?
---

The L1 data cache is the private memory of the
[Streaming Multiprocessor](/gpu-glossary/device-hardware/streaming-multiprocessor)
(SM).

![The internal architecture of an H100 SM. The L1 data cache is depicted in light blue. Modified from NVIDIA's [H100 white paper](https://resources.nvidia.com/en-us-tensor-core).](themed-image://gh100-sm.svg)

Each SM partitions that memory among
[groups of threads](/gpu-glossary/device-software/thread-block) scheduled onto
it.

The L1 data cache is co-located with and nearly as fast as components that
effect computations (e.g. the
[CUDA Cores](/gpu-glossary/device-hardware/cuda-core)).

It is implemented with SRAM, the same basic semiconductor cell used in CPU
caches and registers and in the
[memory subsystem of Groq LPUs](https://groq.com/wp-content/uploads/2023/05/GroqISCAPaper2022_ASoftwareDefinedTensorStreamingMultiprocessorForLargeScaleMachineLearning-1.pdf).
The L1 data cache is accessed by the
[Load/Store Units](/gpu-glossary/device-hardware/load-store-unit) of the
[SM](/gpu-glossary/device-hardware/streaming-multiprocessor).

CPUs also maintain an L1 cache. In CPUs, that cache is fully hardware-managed.
In GPUs that cache is mostly programmer-managed, even in high-level languages
like [CUDA C](/gpu-glossary/host-software/cuda-c).

Each L1 data cache in an each of an H100's SMs can store 256 KiB (2,097,152
bits). Across the 132 SMs in an H100 SXM 5, that's 33 MiB (242,221,056 bits) of
cache space.



================================================
FILE: gpu-glossary/device-hardware/load-store-unit.md
================================================
---
title: What is a Load/Store Unit?
abbreviation: LSU
---

The Load/Store Units (LSUs) dispatch requests to load or store data to the
memory subsystems of the GPU.

![The internal architecture of an H100 SM. Load/Store Units are shown in pink, along with the [Special Function Units](/gpu-glossary/device-hardware/special-function-unit). Modified from NVIDIA's [H100 white paper](https://resources.nvidia.com/en-us-tensor-core).](themed-image://gh100-sm.svg)

Most importantly for
[CUDA programmers](/gpu-glossary/host-software/cuda-software-platform) they
interact with the
[Streaming Multiprocessor](/gpu-glossary/device-hardware/streaming-multiprocessor)'s
on-chip SRAM [L1 data cache](/gpu-glossary/device-hardware/l1-data-cache) and
the off-chip, on-device [global RAM](/gpu-glossary/device-hardware/gpu-ram) that
respectively implement the lowest and highest levels of the
[memory hierarchy](/gpu-glossary/device-software/memory-hierarchy) in the
[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model).



================================================
FILE: gpu-glossary/device-hardware/register-file.md
================================================
---
title: What is a Register File?
---

The register file of the
[Streaming Multiprocessor](/gpu-glossary/device-hardware/streaming-multiprocessor)
stores bits in between their manipulation by the
[cores](/gpu-glossary/device-hardware/core).

![The internal architecture of an H100 SM. The register file is depicted in blue. Modified from NVIDIA's [H100 white paper](https://resources.nvidia.com/en-us-tensor-core).](themed-image://gh100-sm.svg)

The register file is split into 32 bit registers that can be dynamically
reallocated between different data types, like 32 bit integers, 64 bit floating
point numbers, and (pairs of) 16 bit floating point numbers.

Allocation of registers in a
[Streaming Multiprocessor](/gpu-glossary/device-hardware/streaming-multiprocessor)
to [threads](/gpu-glossary/device-software/thread) is therefore generally
managed by a compiler like [nvcc](/gpu-glossary/host-software/nvcc), which
optimizes register usage by
[thread blocks](/gpu-glossary/device-software/thread-block).



================================================
FILE: gpu-glossary/device-hardware/special-function-unit.md
================================================
---
title: What is a Special Function Unit?
abbreviation: SFU
---

The Special Function Units (SFUs) in
[Streaming Multiprocessors (SMs)](/gpu-glossary/device-hardware/streaming-multiprocessor)
accelerate certain arithmetic operations.

![The internal architecture of an H100 SM. Special Function Units are shown in maroon, along with the [Load/Store Units](/gpu-glossary/device-hardware/load-store-unit). Modified from NVIDIA's [H100 white paper](https://resources.nvidia.com/en-us-tensor-core).](themed-image://gh100-sm.svg)

Notable for neural network workloads are transcendental mathematical operations,
like `exp`, `sin`, and `cos`.



================================================
FILE: gpu-glossary/device-hardware/streaming-multiprocessor-architecture.md
================================================
---
title: What is a Streaming Multiprocessor Architecture?
---

[Streaming Multiprocessors (SMs)](/gpu-glossary/device-hardware/streaming-multiprocessor)
are versioned with a particular "architecture" that defines their compatibility
with
[Streaming Assembler (SASS)](/gpu-glossary/device-software/streaming-assembler)
code.

![A streaming multiprocessor with the "Hopper" SM90 architecture. Modified from NVIDIA's [H100 white paper](https://resources.nvidia.com/en-us-tensor-core).](themed-image://gh100-sm.svg)

![A streaming multiprocessor with the original "Tesla" SM architecture. Modified from [Fabien Sanglard's blog](https://fabiensanglard.net/cuda)](themed-image://tesla-sm.svg)

Most [SM](/gpu-glossary/device-hardware/streaming-multiprocessor) versions have
two components: a major version and a minor version.

The major version is _almost_ synonymous with GPU architecture family. For
example, all SM versions `6.x` are of the Pascal Architecture. Some NVIDIA
documentation even
[makes this claim directly](https://docs.nvidia.com/cuda/ptx-writers-guide-to-interoperability/index.html).
But, as an example, Ada GPUs have
[SM](/gpu-glossary/device-hardware/streaming-multiprocessor) architecture
version `8.9`, the same major version as Ampere GPUs.

Target [SM](/gpu-glossary/device-hardware/streaming-multiprocessor) versions for
[SASS](/gpu-glossary/device-software/streaming-assembler) compilation can be
specified when invoking `nvcc`, the
[NVIDIA CUDA Compiler Driver](/gpu-glossary/host-software/nvcc). Compatibility
across major versions is explicitly not guaranteed. For more on compatibility
across minor versions, see the
[documentation](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#gpu-feature-list)
for [nvcc](/gpu-glossary/host-software/nvcc).



================================================
FILE: gpu-glossary/device-hardware/streaming-multiprocessor.md
================================================
---
title: What is a Streaming Multiprocessor?
abbreviation: SM
---

When we [program GPUs](/gpu-glossary/host-software/cuda-software-platform), we
produce
[sequences of instructions](/gpu-glossary/device-software/streaming-assembler)
for its Streaming Multiprocessors to carry out.

![A diagram of the internal architecture of an H100 GPU's Streaming Multiprocessors. GPU cores appear in green, other compute units in maroon, scheduling units in orange, and memory in blue. Modified from NVIDIA's [H100 white paper](https://resources.nvidia.com/en-us-tensor-core).](themed-image://gh100-sm.svg)

Streaming Multiprocessors (SMs) of NVIDIA GPUs are roughly analogous to the
cores of CPUs. That is, SMs both execute computations and store state available
for computation in registers, with associated caches. Compared to CPU cores, GPU
SMs are simple, weak processors. Execution in SMs is pipelined within an
instruction (as in almost all CPUs since the 1990s) but there is no speculative
execution or instruction pointer prediction (unlike all contemporary
high-performance CPUs).

However, GPU SMs can execute more
[threads](/gpu-glossary/device-software/thread) in parallel.

For comparison: an
[AMD EPYC 9965](https://www.techpowerup.com/cpu-specs/epyc-9965.c3904) CPU draws
at most 500 W and has 192 cores, each of which can execute instructions for at
most two threads at a time, for a total of 384 threads in parallel, running at
about 1.25 W per thread.

An H100 SXM GPU draws at most 700 W and has 132 SMs, each of which has four
[Warp Schedulers](/gpu-glossary/device-hardware/warp-scheduler) that can each
issue instructions to 32 threads (aka a
[warp](/gpu-glossary/device-software/warp)) in parallel per clock cycle, for a
total of 128 × 132 > 16,000 parallel threads running at about 5 cW apiece. Note
that this is truly parallel: each of the 16,000 threads can make progress with
each clock cycle.

GPU SMs also support a large number of _concurrent_ threads -- threads of
execution whose instructions are interleaved.

A single SM on an H100 can concurrently execute up to 2048 threads split across
64 thread groups of 32 threads each. With 132 SMs, that's a total of over
250,000 concurrent threads.

CPUs can also run many threads concurrently. But switches between
[warps](/gpu-glossary/device-software/warp) happen at the speed of a single
clock cycle (over 1000x faster than context switches on a CPU), again powered by
the SM's [Warp Schedulers](/gpu-glossary/device-hardware/warp-scheduler). The
volume of available [warps](/gpu-glossary/device-software/warp) and the speed of
warp switches help hide latency caused by memory reads, thread synchronization,
or other expensive instructions, ensuring that the compute resources (especially
the [CUDA Cores](/gpu-glossary/device-hardware/cuda-core) and
[Tensor Cores](/gpu-glossary/device-hardware/tensor-core)) are well utilized.

This latency-hiding is the secret to GPUs' strengths. CPUs seek to hide latency
from end-users and programmers by maintaining large, hardware-managed caches and
sophisticated instruction prediction. This extra hardware limits the fraction of
their silicon area, power, and heat budgets that CPUs can allocate to
computation.

![GPUs dedicate more of their area to compute (green), and less to control and caching (orange and blue), than do CPUs. Modified from a diagram in [Fabien Sanglard's blog](https://fabiensanglard.net/cuda), itself likely modifed from a diagram in [the CUDA C Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/).](themed-image://cpu-vs-gpu.svg)

For programs or functions like neural network inference or sequential database
scans for which it is relatively straightforward for programmers to
[express](/gpu-glossary/device-software/cuda-programming-model) the behavior of
[caches](/gpu-glossary/device-hardware/l1-data-cache) — e.g. store a chunk of
each input matrix and keep it in cache for long enough to compute the related
outputs — the result is much higher throughput.



================================================
FILE: gpu-glossary/device-hardware/tensor-core.md
================================================
---
title: What is a Tensor Core?
---

Tensor Cores are GPU [cores](/gpu-glossary/device-hardware/core) that operate on
entire matrices with each instruction.

![The internal architecture of an H100 SM. Note the larger size and lower number of Tensor Cores. Modified from NVIDIA's [H100 white paper](https://resources.nvidia.com/en-us-tensor-core).](themed-image://gh100-sm.svg)

Operating on more data for a single instruction fetch dramatically reduces power
requirements, which unlocks increased performance (see
[this talk](https://youtu.be/kLiwvnr4L80?t=868) by Bill Dally, Chief Scientist
at NVIDIA). As of the Blackwell
[Streaming Multiprocessor (SM) Architecture](/gpu-glossary/device-hardware/streaming-multiprocessor-architecture)
generation, they are the only way to achieve the highest arithmetic throughput
on NVIDIA GPUs.

As an example, the `HMMA16.16816.F32`
[SASS](/gpu-glossary/device-software/streaming-assembler) instruction calculates
D = AB + C for matrices A, B, C, and D (where C is often the same physical
matrix as D). The `MMA` stands for "Matrix Multiply and Accumulate". `HMMA16`
indicates that the inputs are half-precision (`16` bits) and the `F32` indicates
that the outputs are accumulated into `32` bit (aka single-precision) floats.

`16816` is not single number larger than 16,000. Instead, the string of numbers
`16`, `8`, `16` denote the dimensions of the matrices. These dimensions are
generally named `m`, `k`, and `n` by NVIDIA, for example in
[PTX](/gpu-glossary/device-software/parallel-thread-execution) instructions. The
outer dimensions of A and B, aka `m` and `n`, come first and last, respectively,
and the shared inner dimension for the accumulation, `k`, is in the middle.
Multiplying these out, we see that the `HMMA16.16816.32` instruction performs 16
× 8 × 8 × 16 = 16,384 multiply-accumulate (MAC) operations.

Note that a single instruction in a single
[thread](/gpu-glossary/device-software/thread) does not produce the entire
matrix multiplication. Instead, the 32 threads of a
[warp](/gpu-glossary/device-software/warp) cooperatively produce the result by
executing the instruction together. Most of the per-instruction power overhead
is in decoding, which is shared across a
[warp](/gpu-glossary/device-software/warp) thanks to the
[warp scheduler](/gpu-glossary/device-hardware/warp-scheduler). But even spread
across those 32 threads, that's 512 = 16,384 ÷ 32 MACs per instruction.

For this reason, it is helpful to think of Tensor Cores, and similar hardware
like the systolic arrays in Google Tensor Processing Units (TPUs), as a form of
[complex instruction set computer (CISC)](https://www.omgwiki.org/ddsf/doku.php?id=ddsf:public:guidebook:06_append:glossary:c:cisc)
hardware. For more on this perspective, applied to TPUs, see
[this talk by computer architect David Patterson](https://youtu.be/fhHAArxwzvQ?t=2072),
who also
[coined the terms CISC and RISC](https://www.semanticscholar.org/paper/4d3a941a5749dbf0dd39554f12597c449c3c07ff).

That assembler-level instruction might be produced by a compiler to implement
[PTX-level](/gpu-glossary/device-software/parallel-thread-execution)
matrix-multiply-and-accumlate instructions like `wmma` (documented
[here](https://docs.nvidia.com/cuda/archive/12.8.0/parallel-thread-execution/index.html#warp-level-matrix-instructions)).
Those instructions also calculate D = AB + C for matrices A, B, C, and D, but
are generally compiled into many individual
[SASS](/gpu-glossary/device-software/streaming-assembler) Tensor Core
instructions that operate on smaller matrices.

These instructions from the
[PTX](/gpu-glossary/device-software/parallel-thread-execution) instruction set
architecture are exposed in the high-level
[CUDA C++ programming language](/gpu-glossary/host-software/cuda-c) as
intrinsics.

In reverse order, a line of [CUDA C++](/gpu-glossary/host-software/cuda-c)
coding a matrix multiplication `C = A @ B`, of two 16 by 16 matrices, like

```cpp
wmma::mma_sync(c, a, b, c);
```

where `c` is initialized to all zeros, and the first appearance indicates it is
also the output, might be compiled by [`nvcc`](/gpu-glossary/host-software/nvcc)
to the [PTX](/gpu-glossary/device-software/parallel-thread-execution)
intermediate representation as

```ptx
wmma.mma.sync.aligned.col.row.m16n16k16.f32.f32 {%f2, %f3, %f4, %f5, %f6, %f7, %f8, %f9}, {%r2, %r3, %r4, %r5, %r6, %r7, %r8, %r9}, {%r10, %r11, %r12, %r13, %r14, %r15, %r16, %r17}, {%f1, %f1, %f1, %f1, %f1, %f1, %f1, %f1};
```

and then finally compiled by `ptxas` to
[SASS](/gpu-glossary/device-software/streaming-assembler) as

```sass
HMMA.1688.F32 R20, R12, R11, RZ   // 1
HMMA.1688.F32 R24, R12, R17, RZ   // 2
HMMA.1688.F32 R20, R14, R16, R20  // 3
HMMA.1688.F32 R24, R14, R18, R24  // 4
```

The operands to each `HMMA` instruction can be read, in order, as
`D = A @ B + C`. For example, instruction 3 uses
[register](/gpu-glossary/device-hardware/register-file) 20 for its output `D`,
registers 14 and 16 for its inputs `A` and `B`, respectively, and re-uses
register 20 for its input `C`, effecting the computation `C += A @ B`.

This program partitions the full 16 by 16 square matrix multiplication into four
separate instructions, each itself a matrix multiplication of a 16 by 8 matrix
with an 8 by 8 matrix. Similarly, programs running large-scale matrix
multiplications must break their work down into smaller matrix multiplications,
like the 16 by 16 square matrix multiplication performed by the `mma_sync` call
we are dissecting. We walk through this program below.

![Register usage in a Tensor Core MMA for C = A @ B. The R11, R17, R16, and R18 registers are used in instructions 1, 2, 3, and 4, respectively. See surrounding text for details.](themed-image://tensor-core-mma.svg)

The first two instructions compute the matrix multiplication of the first eight
columns of the input `a`, from `R12`, with the first eight rows of the input
`b`, from `R11` and `R17`, producing a 16 by 16 matrix, which is stored in `R20`
and `R24`. This is a sort of "outer product": a tall and skinny matrix
mutliplied by a short and wide matrix. (`RZ` is a special-purpose "register"
that contains the value `Z`ero).

The second two instructions compute a similar "outer product" for the second
eight columns of `a` and second eight rows of `b`, accumulating with the output
of the first two instructions to produce the final value in `c`.

Put another way: within a block of eight rows out of eight columns in B and
within an entire column of A, a number of multiplications and additions occur
inside the Tensor Core concurrently, with respect to the instruction, to
implement a matrix multiplication. Each instruction handles all `m` rows of A
for the given block of rows and columns from B. Together, they handle the full
matrix multiplication.

Explore [this compiler output on Godbolt](https://godbolt.org/z/e6cqn8491) if
you want to dive deeper. Note that this is far from a
[utilization-maximizing](https://modal.com/blog/gpu-utilization-guide) matrix
multiplication using Tensor Cores! For that, see
[this worklog by Pranjal Shandkar](https://cudaforfun.substack.com/p/outperforming-cublas-on-h100-a-worklog).

Programming Hopper and Blackwell Tensor Cores for maximum performance cannot be
done in pure CUDA C++, requiring instead
[PTX](/gpu-glossary/device-software/parallel-thread-execution) intrinsics for
both computation and memory. It is generally recommended to instead use existing
kernels from kernel libraries like
[cuBLAS (CUDA Basic Linear Algebra Subroutines)](https://docs.nvidia.com/cuda/cublas/)
or higher-level kernel programming interfaces like
[CUTLASS (CUDA Templates for Linear Algebra Subroutines)](https://github.com/NVIDIA/cutlass).
For an introduction to CUTLASS, see
[this blog post series by Colfax Research](https://research.colfax-intl.com/cutlass-tutorial-wgmma-hopper/).

Tensor Cores are much larger and less numerous than CUDA Cores. An H100 SXM5 has
only four Tensor Cores per
[SM](/gpu-glossary/device-hardware/streaming-multiprocessor), i.e. one per
[Warp Scheduler](/gpu-glossary/device-hardware/warp-scheduler), compared to
hundreds of [CUDA Cores](/gpu-glossary/device-hardware/cuda-core).

Tensor Cores were introduced in the V100 GPU, which represented a major
improvement in the suitability of NVIDIA GPUs for large neural network worloads.
For more, see
[the NVIDIA white paper introducing the V100](https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf).

The internals of Tensor Cores are unknown, and likely differ from
[SM Architecture](/gpu-glossary/device-hardware/streaming-multiprocessor-architecture)
to
[SM Architecture](/gpu-glossary/device-hardware/streaming-multiprocessor-architecture).
They are commonly assumed to be systolic arrays, like TPUs, but there is no
consensus in the microbenchmarking literature.



================================================
FILE: gpu-glossary/device-hardware/texture-processing-cluster.md
================================================
---
title: What is a Texture Processing Cluster?
abbreviation: TPC
---

Generally synonymous with "pair of
[Streaming Multiprocessors](/gpu-glossary/device-hardware/streaming-multiprocessor)".
Rarely encountered in contemporary discussions of GPUs and not mapped onto a
level of the
[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model)'s
[memory hierarchy](/gpu-glossary/device-software/memory-hierarchy) or
[thread hierarchy](/gpu-glossary/device-software/thread-block), unlike
[Graphics/GPU Processing Clusters](/gpu-glossary/device-hardware/graphics-processing-cluster).



================================================
FILE: gpu-glossary/device-hardware/warp-scheduler.md
================================================
---
title: What is a Warp Scheduler?
---

The Warp Scheduler of the
[Streaming Multiprocessor (SM)](/gpu-glossary/device-hardware/streaming-multiprocessor)
decides which group of [threads](/gpu-glossary/device-software/thread) to
execute.

![The internal architecture of an H100 SM. The Warp Scheduler and Dispatch Unit are shown in orange. Modified from NVIDIA's [H100 white paper](https://resources.nvidia.com/en-us-tensor-core).](themed-image://gh100-sm.svg)

These groups of threads, known as [warps](/gpu-glossary/device-software/warp),
are switched out on a per clock cycle basis — roughly one nanosecond.

CPU thread context switches, on the other hand, take few hundred to a few
thousand clock cycles (more like a microsecond than a nanosecond) due to the
need to save the context of one thread and restore the context of another.
Additionally, context switches on CPUs lead to reduced locality, further
reducing performance by increasing cache miss rates (see
[Mogul and Borg, 1991](https://www.researchgate.net/publication/220938995_The_Effect_of_Context_Switches_on_Cache_Performance)).

Because each [thread](/gpu-glossary/device-software/thread) has its own private
[registers](/gpu-glossary/device-software/registers) allocated from the
[register file](/gpu-glossary/device-hardware/register-file) of the
[SM](/gpu-glossary/device-hardware/streaming-multiprocessor), context switches
on the GPU do not require any data movement to save or restore contexts.

Because the [L1 caches](/gpu-glossary/device-hardware/l1-data-cache) on GPUs can
be entirely programmer-managed and are
[shared](/gpu-glossary/device-software/shared-memory) between the
[warps](/gpu-glossary/device-software/warp) scheduled together onto an
[SM](/gpu-glossary/device-hardware/streaming-multiprocessor) (see
[cooperative thread array](/gpu-glossary/device-software/cooperative-thread-array)),
context switches on the GPU have much less impact on cache hit rates. For
details on the interaction between programmer-managed caches and
hardware-managed caches in GPUs, see
[the "Maximize Memory Throughput" section of the CUDA C Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#maximize-memory-throughput)



================================================
FILE: gpu-glossary/device-software/compute-capability.md
================================================
---
title: What is Compute Capability?
---

Instructions in the
[Parallel Thread Execution](/gpu-glossary/device-software/parallel-thread-execution)
instruction set are compatible with only certain physical GPUs. The versioning
system used to abstract away details of physical GPUs from the instruction set
and [compiler](/gpu-glossary/host-software/nvcc) is called "Compute Capability".

Most compute capability version numbers have two components: a major version and
a minor version. NVIDIA promises forward compatibility (old
[PTX](/gpu-glossary/device-software/parallel-thread-execution) code runs on new
GPUs) across both major and minor versions following the
[onion layer](https://docs.nvidia.com/cuda/parallel-thread-execution/#ptx-module-directives-target)
model.

With Hopper, NVIDIA has introduced an additional version suffix, the `a` in
`9.0a`, which includes features that deviate from the onion model: their future
support is not guaranteed.

Target compute capabilities for
[PTX](/gpu-glossary/device-software/parallel-thread-execution) compilation can
be specified when invoking `nvcc`, the
[NVIDIA CUDA Compiler Driver](/gpu-glossary/host-software/nvcc). By default, the
compiler will also generate optimized
[SASS](/gpu-glossary/device-software/streaming-assembler) for the matching
[Streaming Multiprocessor (SM) architecture](/gpu-glossary/device-hardware/streaming-multiprocessor-architecture).
The
[documentation](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#virtual-architectures)
for [nvcc](/gpu-glossary/host-software/nvcc) refers to compute capability as a
"virtual GPU architecture", in contrast to the "physical GPU architecture"
expressed by the [SM](/gpu-glossary/device-hardware/streaming-multiprocessor)
version.

The technical specifications for each compute capability version can be found in
the
[Compute Capability section of the NVIDIA CUDA C Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html?highlight=compute%2520capability#compute-capabilities).



================================================
FILE: gpu-glossary/device-software/cooperative-thread-array.md
================================================
---
title: What is a Cooperative Thread Array?
---

![Cooperative thread arrays correspond to the [thread block](/gpu-glossary/device-software/thread-block) level of the thread block hierarchy in the [CUDA programming model](/gpu-glossary/device-software/cuda-programming-model). Modified from diagrams in NVIDIA's [CUDA Refresher: The CUDA Programming Model](https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/) and the NVIDIA [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model).](themed-image://cuda-programming-model.svg)

A cooperative thread array (CTA) is a collection of threads scheduled onto the
same
[Streaming Multiprocessor (SM)](/gpu-glossary/device-hardware/streaming-multiprocessor).
CTAs are the
[PTX](/gpu-glossary/device-software/parallel-thread-execution)/[SASS](/gpu-glossary/device-software/streaming-assembler)
implementation of the
[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model)'s
[thread blocks](/gpu-glossary/device-software/thread-block). CTAs are composed
of one or more [warps](/gpu-glossary/device-software/warp).

Programmers can direct [threads](/gpu-glossary/device-software/thread) within a
CTA to coordinate with each other. The programmer-managed
[shared memory](/gpu-glossary/device-software/shared-memory), in the
[L1 data cache](/gpu-glossary/device-hardware/l1-data-cache) of the
[SMs](/gpu-glossary/device-hardware/streaming-multiprocessor), makes this
coordination fast. Threads in different CTAs cannot coordinate with each other
via barriers, unlike threads within a CTA, and instead must coordinate via
[global memory](/gpu-glossary/device-software/global-memory), e.g. via atomic
update instructions. Due to driver control over the scheduling of CTAs at
runtime, CTA execution order is indeterminate and blocking a CTA on another CTA
can easily lead to deadlock.

The number of CTAs that can be scheduled onto a single
[SM](/gpu-glossary/device-hardware/streaming-multiprocessor) depends on a number
of factors. Fundamentally, the
[SM](/gpu-glossary/device-hardware/streaming-multiprocessor) has a limited set
of resources — lines in the
[register file](/gpu-glossary/device-hardware/register-file), "slots" for
[warps](/gpu-glossary/device-software/warp), bytes of
[shared memory](/gpu-glossary/device-software/shared-memory) in the
[L1 data cache](/gpu-glossary/device-hardware/l1-data-cache) — and each CTA uses
a certain amount of those resources (as calculated at
[compile](/gpu-glossary/host-software/nvcc) time) when scheduled onto an
[SM](/gpu-glossary/device-hardware/streaming-multiprocessor).



================================================
FILE: gpu-glossary/device-software/cuda-programming-model.md
================================================
---
title: What is the CUDA Programming Model?
---

CUDA stands for _Compute Unified Device Architecture_. Depending on the context,
"CUDA" can refer to multiple distinct things: a
[high-level device architecture](/gpu-glossary/device-hardware/cuda-device-architecture),
a parallel programming model for architectures with that design, or a
[software platform](/gpu-glossary/host-software/cuda-software-platform) that
extends high-level languages like C to add that programming model.

The vision for CUDA is laid out in the
[Lindholm et al., 2008](https://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/lindholm08_tesla.pdf)
white paper. We highly recommend this paper, which is the original source for
many claims, diagrams, and even specific turns of phrase in NVIDIA's
documentation.

Here, we focus on the CUDA _programming model_.

The Compute Unified Device Architecture (CUDA) programming model is a
programming model for programming massively parallel processors.

Per the
[NVIDIA CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#a-scalable-programming-model),
there are three key abstractions in the CUDA programming model:

- **Hierarchy of thread groups**. Programs are executed in threads but can make
  reference to groups of threads in a nested hierarchy, from
  [blocks](/gpu-glossary/device-software/thread-block) to
  [grids](/gpu-glossary/device-software/thread-block-grid).
- **Hierarchy of memories**. Thread groups have access to a
  [memory resource](/gpu-glossary/device-software/memory-hierarchy) for
  communication between [threads](/gpu-glossary/device-software/thread) in the
  group. Accessing the
  [lowest layer](/gpu-glossary/device-software/shared-memory) of the memory
  hierarchy should be
  [nearly as fast as executing an instruction](/gpu-glossary/device-hardware/l1-data-cache).
- **Barrier synchronization.** Thread groups can coordinate execution by means
  of barriers.

The hierarchies of execution and memory and their mapping onto
[device hardware](/gpu-glossary/device-hardware) are summarized in the following
diagram.

![Left: the abstract thread group and memory hierarchies of the CUDA programming model. Right: the matching hardware implementing those abstractions. Modified from diagrams in NVIDIA's [CUDA Refresher: The CUDA Programming Model](https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/) and the NVIDIA [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model).](themed-image://cuda-programming-model.svg)

Together, these three abstractions encourage the expression of programs in a way
that scales transparently as GPU devices scale in their parallel execution
resources.

Put provocatively: this programming model prevents programmers from writing
programs for NVIDIA's
[CUDA-architected](/gpu-glossary/device-hardware/cuda-device-architecture) GPUs
that fail to get faster when the program's user buys a new NVIDIA GPU.

For example, each [thread block](/gpu-glossary/device-software/thread-block) in
a CUDA program can coordinate tightly, but coordination between blocks is
limited. This ensures blocks capture parallelizable components of the program
and can be scheduled in any order — in the terminology of NVIDIA documentation,
the programmer "exposes" this parallelism to the compiler and hardware. When the
program is executed on a new GPU that has more scheduling units (specifically,
more
[Streaming Multiprocessors](/gpu-glossary/device-hardware/streaming-multiprocessor)),
more of these blocks can be executed in parallel.

![A CUDA program with eight [blocks](/gpu-glossary/device-software/thread-block) runs in four sequential steps (waves) on a GPU with two [SMs](/gpu-glossary/device-hardware/streaming-multiprocessor) but in half as many steps on one with twice as many [SMs](/gpu-glossary/device-hardware/streaming-multiprocessor). Modified from the [CUDA Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/).](themed-image://wave-scheduling.svg)

The CUDA programming model abstractions are made available to programmers as
extensions to high-level CPU programming languages, like the
[CUDA C++ extension of C++](/gpu-glossary/host-software/cuda-c). The programming
model is implemented in software by an instruction set architecture
[(Parallel Thread eXecution, or PTX)](/gpu-glossary/device-software/parallel-thread-execution)
and low-level assembly language
[(Streaming Assembler, or SASS)](/gpu-glossary/device-software/streaming-assembler).
For example, the [thread block](/gpu-glossary/device-software/thread-block)
level of the thread hierarchy is implemented via
[cooperative thread arrays](/gpu-glossary/device-software/cooperative-thread-array)
in these languages.



================================================
FILE: gpu-glossary/device-software/global-memory.md
================================================
---
title: What is Global Memory?
---

![Global memory is the highest level of the [memory hierarchy](/gpu-glossary/device-software/memory-hierarchy) in the [CUDA programming model](/gpu-glossary/device-software/cuda-programming-model). It is stored in the [GPU RAM](/gpu-glossary/device-hardware/gpu-ram). Modified from diagrams in NVIDIA's [CUDA Refresher: The CUDA Programming Model](https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/) and the NVIDIA [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model).](themed-image://cuda-programming-model.svg)

As part of the
[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model),
each level of the thread group hierarchy has access to matching memory from the
[memory hierarchy](/gpu-glossary/device-software/memory-hierarchy). This memory
can be used for coordination and communication and is managed by the programmer
(not the hardware or a runtime).

The highest level of that memory hierarchy is the global memory. Global memory
is global in its scope and its lifetime. That is, it is accessible by every
[thread](/gpu-glossary/device-software/thread) in a
[thread block grid](/gpu-glossary/device-software/thread-block-grid) and its
lifetime is as long as the execution of the program.

Access to data structures in the global memory can be synchronized across all
accessors using atomic instructions, as with CPU memory. Within a
[cooperative thread array](/gpu-glossary/device-software/cooperative-thread-array),
access can be more tightly synchronized, e.g. with barriers.

This level of the
[memory hierarchy](/gpu-glossary/device-software/memory-hierarchy) is typically
implemented in the [GPU's RAM](/gpu-glossary/device-hardware/gpu-ram) and
allocated from the host using a memory allocator provided by the
[CUDA Driver API](/gpu-glossary/host-software/cuda-driver-api) or the
[CUDA Runtime API](/gpu-glossary/host-software/cuda-runtime-api).



================================================
FILE: gpu-glossary/device-software/kernel.md
================================================
---
title: What is a Kernel?
---

![A single kernel launch corresponds to a [thread block grid](/gpu-glossary/device-software/thread-block-grid) in the [CUDA programming model](/gpu-glossary/device-software/cuda-programming-model). Modified from diagrams in NVIDIA's [CUDA Refresher: The CUDA Programming Model](https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/) and the NVIDIA [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model).](themed-image://cuda-programming-model.svg)

A kernel is the unit of CUDA code that programmers typically write and compose,
akin to a procedure or function in languages targeting CPUs.

Unlike procedures, a kernel is called ("launched") once and returns once, but is
executed many times, once each by a number of
[threads](/gpu-glossary/device-software/thread). These executions are generally
concurrent (their execution order is non-deterministic) and parallel (they occur
simultaneously on different execution units).

The collection of all threads executing a kernel is organized as a kernel grid —
aka a [thread block grid](/gpu-glossary/device-software/thread-block-grid), the
highest level of the
[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model)'s
thread hierarchy. A kernel grid executes across multiple
[Streaming Multiprocessors (SMs)](/gpu-glossary/device-hardware/streaming-multiprocessor)
and so operates at the scale of the entire GPU. The matching level of the
[memory hierarchy](/gpu-glossary/device-software/memory-hierarchy) is the
[global memory](/gpu-glossary/device-software/global-memory).

In [CUDA C++](/gpu-glossary/host-software/cuda-c), kernels are passed pointers
to [global memory](/gpu-glossary/device-software/global-memory) on the device
when they are invoked by the host and return nothing — they just mutate memory.

To give a flavor for CUDA kernel programming, let's walk through two
implementations of the "hello world" of CUDA kernels: matrix multiplication of
two square matrices, `A` and `B`. The two implementations will differ in how
they map the textbook matrix multiplication algorithm onto the thread hierarchy
and [memory hierarchy](/gpu-glossary/device-software/memory-hierarchy).

In the simplest implementation, inspired by the first matmul kernel in
[Programming Massively Parallel Processors](https://www.amazon.com/dp/0323912311)
(4th edition, Figure 3.11), each [thread](/gpu-glossary/device-software/thread)
does all of the work to compute one element of the output matrix -- loading in
turn each element of a particular `row` of `A` and a particular `col`umn of `B`
into [registers](/gpu-glossary/device-software/registers), multiplying the
paired elements, summing the results, and placing the sum back in
[global memory](/gpu-glossary/device-software/global-memory).

```cpp
__global__ void mm(float* A, float* B, float* C, int N) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < N && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < N; k++) {
            sum += A[row * N + k] * B[k * N + col];
        }
        C[row * N + col] = sum;
    }
}
```

In this kernel, each [thread](/gpu-glossary/device-software/thread) does one
floating point operation (FLOP) per read from
[global memory](/gpu-glossary/device-software/global-memory): a multiply and an
add; a load from `A` and a load from `B`. You'll never
[use the whole GPU](https://modal.com/blog/gpu-utilization-guide) that way,
since the bandwidth of the [CUDA Cores](/gpu-glossary/device-hardware/cuda-core)
in FLOPs/s is much higher than the bandwidth between the
[GPU RAM](/gpu-glossary/device-hardware/gpu-ram) and the
[SMs](/gpu-glossary/device-hardware/streaming-multiprocessor).

We can increase the ratio of FLOPs to reads by more carefully mapping the work
in this algorithm onto the thread hierarchy and
[memory hierarchy](/gpu-glossary/device-software/memory-hierarchy). In the
"tiled" matmul kernel below, inspired by that in Figure 5.9 of the 4th edition
of
[Programming Massively Parallel Processors](https://www.amazon.com/dp/0323912311),
we map the loading of submatrices of `A` and `B` and the computation of
submatrices of `C` onto
[shared memory](/gpu-glossary/device-software/shared-memory) and
[thread blocks](/gpu-glossary/device-software/thread-block) respectively.

```cpp
#define TILE_WIDTH 16

__global__ void mm(float* A, float* B, float* C, int N) {

    // declare variables in shared memory ("smem")
    __shared__ float As[TILE_WIDTH][TILE_WIDTH];
    __shared__ float Bs[TILE_WIDTH][TILE_WIDTH];

    int row = blockIdx.y * TILE_WIDTH + threadIdx.y;
    int col = blockIdx.x * TILE_WIDTH + threadIdx.x;

    float c_output = 0;
    for (int m = 0; m < N/TILE_WIDTH; ++m) {

        // each thread loads one element of A and one of B from global memory into smem
        As[threadIdx.y][threadIdx.x] = A[row * N + (m * TILE_WIDTH + threadIdx.x)];
        Bs[threadIdx.y][threadIdx.x] = B[(m * TILE_WIDTH + threadIdx.y) * N + col];

        // we wait until all threads in the 16x16 block are done loading into smem
        // so that it contains two 16x16 tiles
        __syncthreads();

        // then we loop over the inner dimension,
        // performing 16 multiplies and 16 adds per pair of loads from global memory
        for (int k = 0; k < TILE_WIDTH; ++k) {
            c_output += As[threadIdx.y][k] * Bs[k][threadIdx.x];
        }
        // wait for all threads to finish computing
        // before any start loading the next tile into smem
        __syncthreads();
    }
    C[row * N + col] = c_output;
}
```

For each iteration of the outer loop, which loads two elements, a thread runs 16
iterations of the inner loop, which does a multiply and an add, for 16 FLOPs per
global memory read.

This is still far from a fully optimized kernel for matrix multiplication.
[This worklog by Si Boehm of Anthropic](https://siboehm.com/articles/22/CUDA-MMM)
walks through optimizations that further increase the FLOP to memory read ratio
and map the algorithm even more tightly onto the hardware. Our kernels resemble
his Kernel 1 and Kernel 3; the worklog covers ten kernels.

That worklog and this article only consider writing kernels for execution on the
[CUDA Cores](/gpu-glossary/device-hardware/cuda-core). The absolute fastest
matrix multiplication kernels run instead on
[Tensor Cores](/gpu-glossary/device-hardware/tensor-core).



================================================
FILE: gpu-glossary/device-software/memory-hierarchy.md
================================================
---
title: What is the Memory Hierarchy?
---

![[Shared memory](/gpu-glossary/device-software/shared-memory) and [global memory](/gpu-glossary/device-software/global-memory) are two levels of the memory hierarchy in the [CUDA programming model](/gpu-glossary/device-software/cuda-programming-model) (left), mapping onto the [L1 data cache](/gpu-glossary/device-hardware/l1-data-cache) and [GPU RAM](/gpu-glossary/device-hardware/gpu-ram), respectively. Modified from diagrams in NVIDIA's [CUDA Refresher: The CUDA Programming Model](https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/) and the NVIDIA [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model).](themed-image://cuda-programming-model.svg)

As part of the
[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model),
each level of the thread group hierarchy has access to a distinct block of
memory shared by all threads in a group at that level: a "memory hierarchy" to
match the thread group hierarchy. This memory can be used for coordination and
communication and is managed by the programmer (not the hardware or a runtime).

For a [thread block grid](/gpu-glossary/device-software/thread-block-grid), that
shared memory is in the [GPU's RAM](/gpu-glossary/device-hardware/gpu-ram) and
is known as the [global memory](/gpu-glossary/device-software/global-memory).
Access to this memory can be coordinated with atomic operations and barriers,
but execution order across
[thread blocks](/gpu-glossary/device-software/thread-block) is indeterminate.

For a single [thread](/gpu-glossary/device-software/thread), the memory is a
chunk of the
[Streaming Multiprocessor's (SM's)](/gpu-glossary/device-hardware/streaming-multiprocessor)
[register file](/gpu-glossary/device-hardware/register-file). In keeping with
the memory semantics of the
[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model),
this memory is private.

In between, the [shared memory](/gpu-glossary/device-software/shared-memory) for
the [thread block](/gpu-glossary/device-software/thread-block) level of the
thread hierarchy is stored in the
[L1 data cache](/gpu-glossary/device-hardware/l1-data-cache) of each
[SM](/gpu-glossary/device-hardware/streaming-multiprocessor). Careful management
of this cache — e.g. loading data into it to support the maximum number of
arithmetic operations before new data is loaded — is key to the art of designing
high-performance CUDA [kernels](/gpu-glossary/device-software/kernel).



================================================
FILE: gpu-glossary/device-software/parallel-thread-execution.md
================================================
---
title: What is Parallel Thread Execution?
abbreviation: PTX
---

Parallel Thread eXecution (PTX) is an intermediate representation (IR) for code
that will run on a parallel processor (almost always an NVIDIA GPU). It is one
of the formats output by `nvcc`, the
[NVIDIA CUDA Compiler Driver](/gpu-glossary/host-software/nvcc).

NVIDIA documentation refers to PTX as both a "virtual machine" and an
"instruction set architecture".

From the programmer's perspective, PTX is an instruction set for programming
against a virtual machine model. Programmers or compilers producing PTX can be
confident their program will run with the same semantics on many distinct
physical machines, including machines that do not yet exist. In this way, it is
also similar to CPU instruction set architectures like
[x86_64](https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html),
[aarch64](https://developer.arm.com/documentation/ddi0487/latest/), or
[SPARC](https://www.gaisler.com/doc/sparcv8.pdf).

Unlike those ISAs, PTX is very much an
[intermediate representation](https://en.wikipedia.org/wiki/Intermediate_representation),
like LLVM-IR. The PTX components of a
[CUDA binary](/gpu-glossary/host-software/cuda-binary-utilities) will be
just-in-time (JIT) compiled by the host
[CUDA Drivers](/gpu-glossary/host-software/nvidia-gpu-drivers) into
device-specific [SASS](/gpu-glossary/device-software/streaming-assembler) for
execution.

In the case of NVIDIA GPUs, PTX is forward-compatible: GPUs with a matching or
higher [compute capability](/gpu-glossary/device-software/compute-capability)
version will be able to run the program, thanks to this mechanisn of JIT
compilation.

Some exemplary PTX:

```ptx
.reg .f32 %f<7>;
```

- a compiler directive for the
  PTX-to-[SASS](/gpu-glossary/device-software/streaming-assembler) compiler
  indicating that this kernel consumes seven 32-bit floating point
  [registers](/gpu-glossary/device-software/registers). Registers are
  dynamically allocated to groups of
  [threads](/gpu-glossary/device-software/thread)
  ([warps](/gpu-glossary/device-software/warp)) from the
  [SM](/gpu-glossary/device-hardware/streaming-multiprocessor)'s
  [register file](/gpu-glossary/device-hardware/register-file).

```ptx
fma.rn.f32 %f5, %f4, %f3, 0f3FC00000;
```

- apply a fused multiply-add (`fma`) operation to multiply the contents of
  registers `f3` and `f4` and add the constant `0f3FC00000`, storing the result
  in `f5`. All numbers are in 32 bit floating point representation. The `rn`
  suffix for the FMA operation sets the floating point rounding mode to
  [IEEE 754 "round even"](https://en.wikipedia.org/wiki/IEEE_754) (the default).

```ptx
mov.u32 %r1, %ctaid.x;
mov.u32 %r2, %ntid.x;
mov.u32 %r3, %tid.x;
```

- `mov`e the `x`-axis values of the `c`ooperative `t`hread `a`rray `i`n`d`ex,
  the cooperative thread array dimension index (`ntid`), and the `t`hread
  `i`n`d`ex into three `u32` registers `r1` - `r3`.

The PTX programming model exposes multiple levels of parallelism to the
programmer. These levels map directly onto the hardware through the PTX machine
model, diagrammed below.

![The PTX machine model. Modified from the [PTX documentation](https://docs.nvidia.com/cuda/parallel-thread-execution/#ptx-machine-model).](themed-image://ptx-machine-model.svg)

Notably, in this machine model there is a single instruction unit for multiple
processors. While each processor runs one
[thread](/gpu-glossary/device-software/thread), those threads must execute the
same instructions — hence _parallel_ thread execution, or PTX. They coordinate
with each other through
[shared memory](/gpu-glossary/device-software/shared-memory) and effect
different results by means of private
[registers](/gpu-glossary/device-software/registers).

The documentation for the latest version of PTX is available from NVIDIA
[here](https://docs.nvidia.com/cuda/parallel-thread-execution/). The instruction
sets of PTX are versioned with a number called the
"[compute capability](/gpu-glossary/device-software/compute-capability)", which
is synonymous with "minimum supported
[Streaming Multiprocessor architecture](/gpu-glossary/device-hardware/streaming-multiprocessor-architecture)
version".

Writing in-line PTX by hand is uncommon but not unheard of, similar to writing
in-line `x86_64` assembly, as is done in high-performance vectorized query
operators in analytical databases and in performance-sensitive sections of
operating system kernels. At time of writing in October of 2024, in-line PTX is
the only way to take advantage of some Hopper-specific hardware features like
the `wgmma` and `tma` instructions, as in
[Flash Attention 3](https://arxiv.org/abs/2407.08608) or in the
[Machete w4a16 kernels](https://youtu.be/-4ZkpQ7agXM). Viewing
[CUDA C/C++](/gpu-glossary/host-software/cuda-c),
[SASS](/gpu-glossary/device-software/streaming-assembler), and
[PTX](/gpu-glossary/device-software/parallel-thread-execution) together is
supported on
[Godbolt](https://godbolt.org/#z:OYLghAFBqd5TKALEBjA9gEwKYFFMCWALugE4A0BIEAZgQDbYB2AhgLbYgDkAjF%2BTXRMiAZVQtGIHgBYBQogFUAztgAKAD24AGfgCsp5eiyahUAV0wtyKxqiIEh1ZpgDC6embZMQAJnLOAGQImbAA5TwAjbFJfWQAHdCViByY3Dy9fcgSk%2ByEgkPC2KJifWRtsOxSRIhZSIjTPbz9yyqFq2qJ8sMjo2OsauoaM5oHO4O6i3tKASmt0M1JUTi4AUh8AZjjSFmA2FgBqISWVrQBBE/ON4NQPHH2V9ZdxJRU6h9wLtfXr2%2Bx7x9QSiIhHQ70%2BVyYNwsfweTyBmHoBAiYLOXx%2B0P%2BTzMESMSgA%2BgA3HwAOiQKMu30hv0x5kseNIZmEBA4pPJFzxeOA9HQEQkHP2BPQBEw%2ByUwGwbDYnO5vPoeI4UowEmwSiWEGCRH2AFlyPsNftQrr9QBpXU0bksTUSOJIKwXfYOx1O50u11ul0YJhA/bm9CW/YAKlOus93t9/oDACFyPb3XH4/Hw5qojUzRbNQGXNN7gB2SOx/WRgIAeRcxpEAEkAFofdYAEXWPge%2BbODtDmv1qAASugAO7/Ov7HHoVAAawrmHUxPUgf2RdL5eruHuPkj%2BwgRCQpGwLEwE6nM4A9HOS2XKzXps3Y%2B29cJ9qg0gOh9yx/viQBPWfzs9Lldrjdbjue6TtOK4AKwngu564Je6wtucrb7Iex4EDQoo1EQErMB2Sj7CESwvLUn4kPseyjn8m7BMAuG9mQo77IyOCkPs9iMPsACOZjGPYABelopAWaEQN2faYtqK4AGxrBJ96PrCBrZiseaxg6SYsWwcRPloxJaFeiGqWQ676gQWnNnqYnGmZaz5quBCKcp%2BmOkQGl/g8g7nGBkYif2Ab7Maf56isYGDr5%2BaeSZvmhAFD7uEFdZ6acTpKfFjkuEFXk9j5BrRWkcVPtatqzs5mnWUO2A1LOaWed5s5RaVMX0HFCUOslnw5nWXCzPQ3Bgfw3hcDo5DoNwLgKHWiVpaupVKPMiwwhsfDkEQ2idbMSBAb0ECzKOsTEgAnAdh1HUdEmGNw0h9StQ3cPwSggFoS0rbMcCwCgGAaQw0SUNQ71xJ9MRMASqCoDwPA5uQOAEgQSwAGoENgvbFnEzBXXQ9CYaQd0QBEV0RMERHcIt70cMIxZMPQ75XTgewmJIA38IQ26VASKpXdg6gVGYmGE/wGrYN19OGEi2ykO%2Bbg4FdRCkMyPOzOaOxKHDCNIyjvD8IIwhiMqUiyBrigqBoV36DwhjGKYFhWIiER3ZAszoHEuRetwAC0xbrPszt1qEda4JGCgAOIe874ohNsmHOxgOBubUqBkvWv1c9geIABynZH2BuelCeYSnp1KFKqd5c7zsO%2BoLBKM79uO0obnOwSHvwm5qDWdZYO3fzFSO04TCuO4jQGIE4yFMUBjZMkQhDN4Jtj47XTD70JstI77SDH3GSLx3rRMCvYwFD0MSL6Mk8GECHRz/vUizDNCxLL4XU9ZdgvDVwBowy4Lj7KDxI5jp674MQhkvg%2BGmPwZa9NphrQ2jELa5AdqlH2sdRBB1ToCwuuQfqg1n63Xuo9cB5AXrIDQFgPAhASAUCoLQT6rAOA8zkJrcQkgZB0P1moTQgt9B%2BCMCYNAFtrCby7hAZwx8/CDz3pMEo8REjj1SGvJoWQpGzyHhfUofDbDLyPrIzIS8qijHPuIvop9V7pDkYY3eEwR4zDmDfZYXwtg7D2IcSEGdUSoghFCO4sJnivCIGyNx1JPHwgcL4yk7iYQAnhFbYJ6IPEAmxLiQkJI44fFcSE/xsS6QMiZCyJJnwzgci5DyPkeIBRChFGKCUUoCmynlBKPEjIlSMFVNgTA6o7w6lvJqQ0HS/Jpj9FaegNo7SOQTCMkZN41JBhDEIMM6ZAzRhUqMxZcY1IpisD6WZmZ7LwQdIWU8i4axuUbM1e80yOx3hqm5Z8I5xwgRnKFPZ0EAoAW3LuN8M41gQW/PsmCxybydjkvWK5r4QKfnuVBX8pVnlATeUhSCP4LwJVjMhPUaFT6YRJjhPCFQVRKCIixdApEWDkRYkgKiNE6IMR7tEFin0OJcSZHxR2gl1wXMeOJaS0lZLuDEqELZCy1LFS0jpY5BlmKtI7KZOC5l5KWSlS3WyfLhnqRKlNQFHkMqiV8v5UqBA8qhXShFbK9VcrBRFbmFKiVHRVQ1VlOqq4uWNWCvlAZhVfKCtKqsyqWdMq1RyrFU1cFYytVRO1TqZ0uC9XQVdZ%2Bo1xr7EmjZNc185orkbKAp6kDdybW2rtJBSCUHnX4GwEA6wf5gSjU/G61gcFgJ0M9RAEA3roA%2BowchP1m1/VbSAQGwNQbg0htDbAStEbIwwerBgGMsY40FnjVgotaHE2wmTCmVMJRmzpoNRmncCAszuoLdmnNuZq0oMIfmV0rYizFsQyW0ti1qzlkYaiw6Vb9UWnrLWjDdbyGUKwo2mQuHm0sELa28A7YOxSHu127tPbe19gHIOIdoiWmwBHYh0dFhxzrNnJOqdyDp0ztVDtidc7WALlJJ1xdS7l0ruB6Ztd67O0bvHeVkY26qO3Y4QRPdj4m1EeYhe8icgpB44J6ReiR4bzUTojoIntFtF0Uo/Rh8ZOaOU3UcTC8r6zVvjwe%2BEbH6YO4K/d%2Bn8eDf1/hAf%2BZDU26fTeAzNTFqA5vgXmxBBauBoLHddLg2CHq1tWuGnwRaQBgW0snHge0JI5h8DwdYoNpA5jAuWrzWDcF1vwQ2htKAB1LG%2BsJDt/1QjsGWKEN%2BH8v4/0Gs0gB0tMAGHfQwzgTC9Y/sNuwrR/CUjd17sYgePcNMH1E47ETM8UgDZPp1%2BTKneuSY49vBTYiJP9Gm/3NTZj54H1mFLbA2BhT3T05GlL3A6zYChksfYw7qWlZMxVizVnAELV1G4Ft1KgEgLSwF9aWboE5ukBJYkElwtA54MDoH4a0HFp4FoB6R2fPVr809DL8BG0gBy9gPLv1CvFe4Nd8rZnKv8Gq2QvbfgGva2a9%2Bg2bDBrG3Y1vbrIm%2BMbdHgo4TqmhtjcU0tuT82Vvrzp%2Bos%2BXOBOmNkwt/jm2lrbl23VrQB2DP8Gfids7fxLvMVx6Z8zWg/6kIe42J7BXW02fe/5iB5AvuOZgTtMC6xiRxdByDx3ydOGFvIJD6HFbDNw7ugj%2BzgXgvSD2sSEHWhGw5lTsnZOYFSgSVOrDuzdac1Q4egLdYCvvMJ4CyzTGXXpBAA%3D%3D).
See the
[NVIDIA "Inline PTX Assembly in CUDA" guide](https://docs.nvidia.com/cuda/inline-ptx-assembly/)
for details.



================================================
FILE: gpu-glossary/device-software/registers.md
================================================
---
title: What are Registers?
---

![Registers are the memory of the [memory hierarchy](/gpu-glossary/device-software/memory-hierarchy) associated with individual [threads](/gpu-glossary/device-software/thread) (left). Modified from diagrams in NVIDIA's [CUDA Refresher: The CUDA Programming Model](https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/) and the NVIDIA [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model).](themed-image://cuda-programming-model.svg)

At the lowest level of the
[memory hierarchy](/gpu-glossary/device-software/memory-hierarchy) are the
registers, which store information manipulated by a single
[thread](/gpu-glossary/device-software/thread).

The values in registers are generally stored in the
[register file](/gpu-glossary/device-hardware/register-file) of the
[Streaming Multiprocessor (SM)](/gpu-glossary/device-hardware/streaming-multiprocessor),
but they can also spill to the
[global memory](/gpu-glossary/device-software/global-memory) in the
[GPU RAM](/gpu-glossary/device-hardware/gpu-ram) at a substantial performance
penalty.

As when programming CPUs, these registers are not directly manipulated by
high-level languages like [CUDA C](/gpu-glossary/host-software/cuda-c). They are
only visible to lower-level languages like
[Parallel Thread Execution (PTX)](/gpu-glossary/device-software/parallel-thread-execution)
or
[Streaming Assembler (SASS)](/gpu-glossary/device-software/streaming-assembler)
and so are typically managed by a compiler like
[nvcc](/gpu-glossary/host-software/nvcc). Among the compiler's goals is to limit
the register space used by each [thread](/gpu-glossary/device-software/thread)
so that more [thread blocks](/gpu-glossary/device-software/thread-block) can be
simultaneously scheduled into a single
[SM](/gpu-glossary/device-hardware/streaming-multiprocessor).

The registers used in the
[PTX](/gpu-glossary/device-software/parallel-thread-execution) instruction set
architecture are documented
[here](https://docs.nvidia.com/cuda/parallel-thread-execution/#register-state-space).
The registers used in [SASS](/gpu-glossary/device-software/streaming-assembler)
are not, to our knowledge, documented.



================================================
FILE: gpu-glossary/device-software/shared-memory.md
================================================
---
title: What is Shared Memory?
---

![Shared memory is the abstract memory associated with the [thread block](/gpu-glossary/device-software/thread-block) level (left, center) of the CUDA thread group hierarchy (left). Modified from diagrams in NVIDIA's [CUDA Refresher: The CUDA Programming Model](https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/) and the NVIDIA [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model).](themed-image://cuda-programming-model.svg)

Shared memory is the level of the
[memory hierarchy](/gpu-glossary/device-software/memory-hierarchy) corresponding
to the [thread block](/gpu-glossary/device-software/thread-block) level of the
thread group hierarchy in the
[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model).
It is generally expected to be much smaller but much faster (in throughput and
latency) than the [global memory](/gpu-glossary/device-software/global-memory).

A fairly typical [kernel](/gpu-glossary/device-software/kernel) therefore looks
something like this:

- load data from [global memory](/gpu-glossary/device-software/global-memory)
  into shared memory
- perform a number of arithmetic operations on that data via the
  [CUDA Cores](/gpu-glossary/device-hardware/cuda-core) and
  [Tensor Cores](/gpu-glossary/device-hardware/tensor-core)
- optionally, synchronize [threads](/gpu-glossary/device-software/thread) within
  a [thread block](/gpu-glossary/device-software/thread-block) by means of
  barriers while performing those operations
- write data back into
  [global memory](/gpu-glossary/device-software/global-memory), optionally
  preventing races across
  [thread blocks](/gpu-glossary/device-software/thread-block) by means of
  atomics

Shared memory is stored in the
[L1 data cache](/gpu-glossary/device-hardware/l1-data-cache) of the GPU's
[Streaming Multiprocessor (SM)](/gpu-glossary/device-hardware/streaming-multiprocessor).



================================================
FILE: gpu-glossary/device-software/streaming-assembler.md
================================================
---
title: What is Streaming Assembler?
abbreviation: SASS
---

[Streaming ASSembler](https://stackoverflow.com/questions/9798258/what-is-sass-short-for)
(SASS) is the assembly format for programs running on NVIDIA GPUs. This is the
lowest-level format in which human-readable code can be written. It is one of
the formats output by `nvcc`, the
[NVIDIA CUDA Compiler Driver](/gpu-glossary/host-software/nvcc), alongside
[PTX](/gpu-glossary/device-software/parallel-thread-execution). It is converted
to device-specific binary microcodes during execution. Presumably, the
"Streaming" in "Streaming Assembler" refers to the
[Streaming Multiprocessors](/gpu-glossary/device-hardware/streaming-multiprocessor)
which the assembly language programs.

SASS is versioned and tied to a specific NVIDIA GPU
[SM architecture](/gpu-glossary/device-hardware/streaming-multiprocessor-architecture).
See also [Compute Capability](/gpu-glossary/device-software/compute-capability).

Some exemplary instructions in SASS for the SM90a architecture of Hopper GPUs:

- `FFMA R0, R7, R0, 1.5 ;` - perform a `F`used `F`loating point `M`ultiply `A`dd
  that multiplies the contents of `R`egister 7 and `R`egister 0, adds `1.5`, and
  stores the result in `R`egister 0.
- `S2UR UR4, SR_CTAID.X ;` - copy the `X` value of the
  [Cooperative Thread Array](/gpu-glossary/device-software/cooperative-thread-array)'s
  `I`n`D`ex from its `S`pecial `R`egister to `U`niform `R`egister 4.

As for CPUs, writing this "GPU assembler" by hand is very uncommon. Viewing
compiler-generated SASS while profiling and editing high-level
[CUDA C/C++](/gpu-glossary/host-software/cuda-c) code or in-line
[PTX](/gpu-glossary/device-software/parallel-thread-execution) is
[more common](https://docs.nvidia.com/gameworks/content/developertools/desktop/ptx_sass_assembly_debugging.htm),
especially in the production of the highest-performance kernels. Viewing
[CUDA C/C++](/gpu-glossary/host-software/cuda-c), SASS, and PTX together is
supported on
[Godbolt](https://godbolt.org/#z:OYLghAFBqd5TKALEBjA9gEwKYFFMCWALugE4A0BIEAZgQDbYB2AhgLbYgDkAjF%2BTXRMiAZVQtGIHgBYBQogFUAztgAKAD24AGfgCsp5eiyahUAV0wtyKxqiIEh1ZpgDC6embZMQAJnLOAGQImbAA5TwAjbFJfWQAHdCViByY3Dy9fcgSk%2ByEgkPC2KJifWRtsOxSRIhZSIjTPbz9yyqFq2qJ8sMjo2OsauoaM5oHO4O6i3tKASmt0M1JUTi4AUh8AZjjSFmA2FgBqISWVrQBBE/ON4NQPHH2V9ZdxJRU6h9wLtfXr2%2Bx7x9QSiIhHQ70%2BVyYNwsfweTyBmHoBAiYLOXx%2B0P%2BTzMESMSgA%2BgA3HwAOiQKMu30hv0x5kseNIZmEBA4pPJFzxeOA9HQEQkHP2BPQBEw%2ByUwGwbDYnO5vPoeI4UowEmwSiWEGCRH2AFlyPsNftQrr9QBpXU0bksTUSOJIKwXfYOx1O50u11ul0YJhA/bm9CW/YAKlOus93t9/oDACFyPb3XH4/Hw5qojUzRbNQGXNN7gB2SOx/WRgIAeRcxpEAEkAFofdYAEXWPge%2BbODtDmv1qAASugAO7/Ov7HHoVAAawrmHUxPUgf2RdL5eruHuPkj%2BwgRCQpGwLEwE6nM4A9HOS2XKzXps3Y%2B29cJ9qg0gOh9yx/viQBPWfzs9Lldrjdbjue6TtOK4AKwngu564Je6wtucrb7Iex4EDQoo1EQErMB2Sj7CESwvLUn4kPseyjn8m7BMAuG9mQo77IyOCkPs9iMPsACOZjGPYABelopAWaEQN2faYtqK4AGxrBJ96PrCBrZiseaxg6SYsWwcRPloxJaFeiGqWQ676gQWnNnqYnGmZaz5quBCKcp%2BmOkQGl/g8g7nGBkYif2Ab7Maf56isYGDr5%2BaeSZvmhAFD7uEFdZ6acTpKfFjkuEFXk9j5BrRWkcVPtatqzs5mnWUO2A1LOaWed5s5RaVMX0HFCUOslnw5nWXCzPQ3Bgfw3hcDo5DoNwLgKHWiVpaupVKPMiwwhsfDkEQ2idbMSBAb0ECzKOsTEgAnAdh1HUdEmGNw0h9StQ3cPwSggFoS0rbMcCwCgGAaQw0SUNQ71xJ9MRMASqCoDwPA5uQOAEgQSwAGoENgvbFnEzBXXQ9CYaQd0QBEV0RMERHcIt70cMIxZMPQ75XTgewmJIA38IQ26VASKpXdg6gVGYmGE/wGrYN19OGEi2ykO%2Bbg4FdRCkMyPOzOaOxKHDCNIyjvD8IIwhiMqUiyBrigqBoV36DwhjGKYFhWIiER3ZAszoHEuRetwAC0xbrPszt1qEda4JGCgAOIe874ohNsmHOxgOBubUqBkvWv1c9geIABynZH2BuelCeYSnp1KFKqd5c7zsO%2BoLBKM79uO0obnOwSHvwm5qDWdZYO3fzFSO04TCuO4jQGIE4yFMUBjZMkQhDN4Jtj47XTD70JstI77SDH3GSLx3rRMCvYwFD0MSL6Mk8GECHRz/vUizDNCxLL4XU9ZdgvDVwBowy4Lj7KDxI5jp674MQhkvg%2BGmPwZa9NphrQ2jELa5AdqlH2sdRBB1ToCwuuQfqg1n63Xuo9cB5AXrIDQFgPAhASAUCoLQT6rAOA8zkJrcQkgZB0P1moTQgt9B%2BCMCYNAFtrCby7hAZwx8/CDz3pMEo8REjj1SGvJoWQpGzyHhfUofDbDLyPrIzIS8qijHPuIvop9V7pDkYY3eEwR4zDmDfZYXwtg7D2IcSEGdUSoghFCO4sJnivCIGyNx1JPHwgcL4yk7iYQAnhFbYJ6IPEAmxLiQkJI44fFcSE/xsS6QMiZCyJJnwzgci5DyPkeIBRChFGKCUUoCmynlBKPEjIlSMFVNgTA6o7w6lvJqQ0HS/Jpj9FaegNo7SOQTCMkZN41JBhDEIMM6ZAzRhUqMxZcY1IpisD6WZmZ7LwQdIWU8i4axuUbM1e80yOx3hqm5Z8I5xwgRnKFPZ0EAoAW3LuN8M41gQW/PsmCxybydjkvWK5r4QKfnuVBX8pVnlATeUhSCP4LwJVjMhPUaFT6YRJjhPCFQVRKCIixdApEWDkRYkgKiNE6IMR7tEFin0OJcSZHxR2gl1wXMeOJaS0lZLuDEqELZCy1LFS0jpY5BlmKtI7KZOC5l5KWSlS3WyfLhnqRKlNQFHkMqiV8v5UqBA8qhXShFbK9VcrBRFbmFKiVHRVQ1VlOqq4uWNWCvlAZhVfKCtKqsyqWdMq1RyrFU1cFYytVRO1TqZ0uC9XQVdZ%2Bo1xr7EmjZNc185orkbKAp6kDdybW2rtJBSCUHnX4GwEA6wf5gSjU/G61gcFgJ0M9RAEA3roA%2BowchP1m1/VbSAQGwNQbg0htDbAStEbIwwerBgGMsY40FnjVgotaHE2wmTCmVMJRmzpoNRmncCAszuoLdmnNuZq0oMIfmV0rYizFsQyW0ti1qzlkYaiw6Vb9UWnrLWjDdbyGUKwo2mQuHm0sELa28A7YOxSHu127tPbe19gHIOIdoiWmwBHYh0dFhxzrNnJOqdyDp0ztVDtidc7WALlJJ1xdS7l0ruB6Ztd67O0bvHeVkY26qO3Y4QRPdj4m1EeYhe8icgpB44J6ReiR4bzUTojoIntFtF0Uo/Rh8ZOaOU3UcTC8r6zVvjwe%2BEbH6YO4K/d%2Bn8eDf1/hAf%2BZDU26fTeAzNTFqA5vgXmxBBauBoLHddLg2CHq1tWuGnwRaQBgW0snHge0JI5h8DwdYoNpA5jAuWrzWDcF1vwQ2htKAB1LG%2BsJDt/1QjsGWKEN%2BH8v4/0Gs0gB0tMAGHfQwzgTC9Y/sNuwrR/CUjd17sYgePcNMH1E47ETM8UgDZPp1%2BTKneuSY49vBTYiJP9Gm/3NTZj54H1mFLbA2BhT3T05GlL3A6zYChksfYw7qWlZMxVizVnAELV1G4Ft1KgEgLSwF9aWboE5ukBJYkElwtA54MDoH4a0HFp4FoB6R2fPVr809DL8BG0gBy9gPLv1CvFe4Nd8rZnKv8Gq2QvbfgGva2a9%2Bg2bDBrG3Y1vbrIm%2BMbdHgo4TqmhtjcU0tuT82Vvrzp%2Bos%2BXOBOmNkwt/jm2lrbl23VrQB2DP8Gfids7fxLvMVx6Z8zWg/6kIe42J7BXW02fe/5iB5AvuOZgTtMC6xiRxdByDx3ydOGFvIJD6HFbDNw7ugj%2BzgXgvSD2sSEHWhGw5lTsnZOYFSgSVOrDuzdac1Q4egLdYCvvMJ4CyzTGXXpBAA%3D%3D).
For more detail on SASS with a focus on performance debugging workflows, see
[this talk](https://www.youtube.com/watch?v=we3i5VuoPWk) from Arun Demeure.

SASS is _very_ lightly documented — the instructions are listed in the
[documentation for NVIDIA's CUDA binary utilities](https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html#instruction-set-ref),
but their semantics are not defined. The mapping from ASCII assembler to binary
opcodes and operands is entirely undocumented, but it has been
reverse-engineered in certain cases
([Maxwell](https://github.com/NervanaSystems/maxas),
[Lovelace](https://kuterdinel.com/nv_isa_sm89/)).



================================================
FILE: gpu-glossary/device-software/thread-block-grid.md
================================================
---
title: What is a Thread Block Grid?
---

![Thread block grids are the highest level of the thread group hierarchy of the [CUDA programming model](/gpu-glossary/device-software/cuda-programming-model) (left). They map onto multiple [Streaming Multiprocessors](/gpu-glossary/device-hardware/streaming-multiprocessor) (right, bottom). Modified from diagrams in NVIDIA's [CUDA Refresher: The CUDA Programming Model](https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/) and the NVIDIA [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model).](themed-image://cuda-programming-model.svg)

When a CUDA [kernel](/gpu-glossary/device-software/kernel) is launched, it
creates a collection of [threads](/gpu-glossary/device-software/thread) known as
a thread block grid. Grids can be one, two, or three dimensional. They are made
up of [thread blocks](/gpu-glossary/device-software/thread-block).

The matching level of the
[memory hierarchy](/gpu-glossary/device-software/memory-hierarchy) is the
[global memory](/gpu-glossary/device-software/global-memory).

[Thread blocks](/gpu-glossary/device-software/thread-block) are effectively
independent units of computation. They execute concurrently, that is, with
indeterminate order, ranging from fully sequentially in the case of a GPU with a
single
[Streaming Multiprocessor](/gpu-glossary/device-hardware/streaming-multiprocessor)
to fully in parallel when run on a GPU with sufficient resources to run them all
simultaneously.



================================================
FILE: gpu-glossary/device-software/thread-block.md
================================================
---
title: What is a Thread Block?
---

![Thread blocks are an intermediate level of the thread group hierarchy of the [CUDA programming model](/gpu-glossary/device-software/cuda-programming-model) (left). A thread block executes on a single [Streaming Multiprocessor](/gpu-glossary/device-hardware/streaming-multiprocessor) (right, middle). Modified from diagrams in NVIDIA's [CUDA Refresher: The CUDA Programming Model](https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/) and the NVIDIA [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model).](themed-image://cuda-programming-model.svg)

A thread block is a level of the CUDA programming model's thread hierarchy below
a grid but above a [warp](/gpu-glossary/device-software/warp). It is the CUDA
programming model's abstract equivalent of the concrete
[cooperative thread arrays](/gpu-glossary/device-software/cooperative-thread-array)
in
[PTX](/gpu-glossary/device-software/parallel-thread-execution)/[SASS](/gpu-glossary/device-software/streaming-assembler).

Blocks are the smallest unit of thread coordination exposed to programmers.
Blocks must execute independently, so that any execution order for blocks is
valid, from fully serial in any order to all interleavings.

A single CUDA [kernel](/gpu-glossary/device-software/kernel) launch produces one
or more thread blocks (in the form of a
[block grid](/gpu-glossary/device-software/thread-block-grid)), each of which
contains one or more [warps](/gpu-glossary/device-software/warp). Blocks can be
arbitrarily sized, but they are typically multiples of the
[warp](/gpu-glossary/device-software/warp) size (32 on all current CUDA GPUs).



================================================
FILE: gpu-glossary/device-software/thread.md
================================================
---
title: What is a Thread?
---

![Threads are the lowest level of the thread group hierarchy (top, left) and are mapped onto the [cores](/gpu-glossary/device-hardware/core) of a [Streaming Multiprocessor](/gpu-glossary/device-hardware/streaming-multiprocessor). Modified from diagrams in NVIDIA's [CUDA Refresher: The CUDA Programming Model](https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/) and the NVIDIA [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model).](themed-image://cuda-programming-model.svg)

A _thread of execution_ (or "thread" for short) is the lowest unit of
programming for GPUs, the atom of the
[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model)'s
thread group hierarchy. A thread has its own
[registers](/gpu-glossary/device-software/registers), but little else.

Both [SASS](/gpu-glossary/device-software/streaming-assembler) and
[PTX](/gpu-glossary/device-software/parallel-thread-execution) programs target
threads. Compare this to a typical C program in a POSIX environment, which
targets a process, itself a collection of one or more threads.

Like a thread on a CPU, a GPU thread can have a private instruction
pointer/program counter. However, for performance reasons, GPU programs are
generally written so that all the threads in a
[warp](/gpu-glossary/device-software/warp) share the same instruction pointer,
executing instructions in lock-step (see also
[Warp Scheduler](/gpu-glossary/device-hardware/warp-scheduler)).

Also like threads on CPUs, GPU threads have stacks in
[global memory](/gpu-glossary/device-hardware/gpu-ram) for storing spilled
registers and a function call stack, but high-performance
[kernels](/gpu-glossary/device-software/kernel) generally avoid using either.

A single [CUDA Core](/gpu-glossary/device-hardware/cuda-core) executes
instructions from a single thread.



================================================
FILE: gpu-glossary/device-software/warp.md
================================================
---
title: What is a Warp?
---

A warp is a group of [threads](/gpu-glossary/device-software/thread) that are
scheduled together and execute in parallel. All threads in a warp are scheduled
onto a single
[Streaming Multiprocessor (SM)](/gpu-glossary/device-hardware/streaming-multiprocessor).
A single [SM](/gpu-glossary/device-hardware/streaming-multiprocessor) typically
executes multiple warps, at the very least all warps from the same
[Cooperative Thread Array](/gpu-glossary/device-software/cooperative-thread-array),
aka [thread block](/gpu-glossary/device-software/thread-block).

Warps are the typical unit of execution on a GPU. In normal execution, all
[threads](/gpu-glossary/device-software/thread) of a warp execute the same
instruction in parallel — the so-called "Single-Instruction, Multiple Thread" or
SIMT model. Warp size is technically a machine-dependent constant, but in
practice it is 32.

When a warp is issued an instruction, the results are generally not available
within a single clock cycle, and so dependent instructions cannot be issued.
While this is most obviously true for fetches from
[global memory](/gpu-glossary/device-software/global-memory), which generally
[go off-chip](/gpu-glossary/device-hardware/gpu-ram), it is also true for some
arithmetic instructions (see
[the CUDA C++ Programing Guide's "Performance Guidelines"](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#arithmetic-instructions-throughput-native-arithmetic-instructions)
for a table of results per clock cycle for specific instructions).

Instead of waiting for a warp to return results, when multiple warps are
scheduled onto a single
[SM](/gpu-glossary/device-hardware/streaming-multiprocessor), the
[Warp Scheduler](/gpu-glossary/device-hardware/warp-scheduler) will select
another warp to execute. This "latency-hiding" is how GPUs achieve high
throughput and ensure work is always available for all of their cores during
execution. For this reason, it is often beneficial to maximize the number of
warps scheduled onto each
[SM](/gpu-glossary/device-hardware/streaming-multiprocessor), ensuring there is
always a warp ready for the
[SM](/gpu-glossary/device-hardware/streaming-multiprocessor) to run.

Warps are not actually part of the
[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model)'s
thread group hierarchy. Instead, they are an implementation detail of the
implementation of that model on NVIDIA GPUs. In that way, they are somewhat akin
to
[cache lines](https://www.nic.uoregon.edu/~khuck/ts/acumem-report/manual_html/ch03s02.html)
in CPUs: a feature of the hardware that you don't directly control and don't
need to consider for program correctness, but which is important for achieving
maximum performance.

Warps are named in reference to weaving, "the first parallel thread technology",
according to
[Lindholm et al., 2008](https://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/lindholm08_tesla.pdf).
The equivalent of warps in other GPU programming models include
[subgroups](https://github.com/gpuweb/gpuweb/pull/4368) in WebGPU,
[waves](https://microsoft.github.io/DirectX-Specs/d3d/HLSL_SM_6_6_WaveSize.html)
in DirectX, and
[simdgroups](https://developer.apple.com/documentation/metal/compute_passes/creating_threads_and_threadgroups#2928931)
in Metal.



================================================
FILE: gpu-glossary/host-software/cuda-binary-utilities.md
================================================
---
title: What are the CUDA Binary Utilities?
---

The CUDA Binary Utilities are a collection of tools for examining the contents
of binaries like those output by `nvcc`, the
[NVIDIA CUDA Compiler driver](/gpu-glossary/host-software/nvcc).

One tool, `cuobjdump`, can be used to examine and manipulate the contents of
entire host binaries or of the CUDA-specific `cubin` files that are normally
embedded within those binaries.

Another, `nvidisasm`, is intended for manipulating `cubin` files. It can extract
[SASS assembler](/gpu-glossary/device-software/streaming-assembler) and
manipulate it, e.g. constructing control flow graphs and mapping assembly
instructions to lines in CUDA program files.

You can find their documentation
[here](https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html).



================================================
FILE: gpu-glossary/host-software/cuda-c.md
================================================
---
title: What is the CUDA C++ programming language?
---

CUDA C++ is an implementation of the
[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model)
as an extension of the C++ programming language.

CUDA C++ adds several features to C++ to implement the
[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model),
including:

- **[Kernel](/gpu-glossary/device-software/kernel) definition** with
  **`global`**. CUDA [kernels](/gpu-glossary/device-software/kernel) are
  implemented as C functions that take in pointers and have return type `void`,
  annotated with this keyword.
- **[Kernel](/gpu-glossary/device-software/kernel) launches** with **`<<<>>>`**.
  [Kernels](/gpu-glossary/device-software/kernel) are executed from the CPU host
  using a triple bracket syntax that sets the
  [thread block grid](/gpu-glossary/device-software/thread-block-grid)
  dimensions.
- **[Shared memory](/gpu-glossary/device-software/shared-memory) allocation**
  with the `shared` keyword, **barrier synchronization** with the
  `__syncthreads()` intrinsic function, and
  **[thread block](/gpu-glossary/device-software/thread-block)** and
  **[thread](/gpu-glossary/device-software/thread) indexing** with the
  `blockDim` and `threadIdx` built-in variables.

CUDA C++ programs are compiled by a combination of host C/C++ compiler drivers
like `gcc` and the
[NVIDIA CUDA Compiler Driver](/gpu-glossary/host-software/nvcc), `nvcc`.

For information on how to use CUDA C++ on Modal, see
[this guide](https://modal.com/docs/guide/cuda).



================================================
FILE: gpu-glossary/host-software/cuda-driver-api.md
================================================
---
title: What is the CUDA Driver API?
---

The [CUDA Driver API](https://docs.nvidia.com/cuda/cuda-driver-api/index.html)
is the userspace component of the NVIDIA CUDA drivers. It provides utilities
familiar to users of the C standard library: a `cuMalloc` function for
allocating [memory](/gpu-glossary/device-software/global-memory) on GPU devices,
for example.

![The CUDA Toolkit. The CUDA Driver API sits between applications or other toolkit components and the GPU. Adapted from the *Professional CUDA C Programming Guide*.](themed-image://cuda-toolkit.svg)

Very few CUDA programs are written to directly use the CUDA Driver API. They
instead use the
[CUDA Runtime API](/gpu-glossary/host-software/cuda-runtime-api). See
[this section](https://docs.nvidia.com/cuda/cuda-driver-api/driver-vs-runtime-api.html#driver-vs-runtime-api)
of the CUDA Driver API docs.

The CUDA Driver API is generally not linked statically. Instead, it is linked
dynamically, typically under the name
[libcuda.so](/gpu-glossary/host-software/libcuda) on Linux systems.

The CUDA Driver API is binary-compatible: an application compiled against old
versions of the CUDA Driver API can run on systems with newer versions of the
CUDA Driver API. That is, the operating system's binary loader may load a newer
version of the CUDA Driver API and the program will function the same.

For details on distributing CUDA C applications, see the
[CUDA C/C++ Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide)
from NVIDIA.

The CUDA Driver API is closed source. You can find its documentation
[here](https://docs.nvidia.com/cuda/cuda-driver-api/index.html).

Though they are not commonly used, there are projects that attempt to provide or
use open source alternatives to the CUDA Driver API, like
[LibreCuda](https://github.com/mikex86/LibreCuda) and
[tinygrad](https://github.com/tinygrad). See
[their source code](https://github.com/tinygrad/tinygrad/blob/77f7ddf62a78218bee7b4f7b9ff925a0e581fcad/tinygrad/runtime/ops_nv.py)
for details.



================================================
FILE: gpu-glossary/host-software/cuda-runtime-api.md
================================================
---
title: What is the CUDA Runtime API?
---

The CUDA Runtime API wraps the
[CUDA Driver API](/gpu-glossary/host-software/cuda-driver-api) and provides a
higher-level API for the same functions.

![The CUDA Toolkit. The CUDA Runtime API wraps the CUDA Driver API to make it more amenable to application programming. Adapted from the *Professional CUDA C Programming Guide*.](themed-image://cuda-toolkit.svg)

It is generally preferred over the
[Driver API](/gpu-glossary/host-software/cuda-driver-api) for better ergonomics,
but there are some small caveats around control of kernel launches and context
management. See
[this section](https://docs.nvidia.com/cuda/cuda-runtime-api/driver-vs-runtime-api.html#driver-vs-runtime-api)
of the CUDA Runtime API docs for more.

While the Runtime API may be statically linked, per
[Attachment A of the NVIDIA CUDA Toolkit EULA](https://docs.nvidia.com/cuda/eula/index.html#attachment-a),
it does not have to be. The shared object file for dynamic linking is usually
named [libcudart.so](/gpu-glossary/host-software/libcudart) on Linux systems.

The CUDA Runtime API is closed source. You can find its documentation
[here](https://docs.nvidia.com/cuda/cuda-runtime-api/index.html).



================================================
FILE: gpu-glossary/host-software/cuda-software-platform.md
================================================
---
title: What is the CUDA Software Platform?
---

CUDA stands for _Compute Unified Device Architecture_. Depending on the context,
"CUDA" can refer to multiple distinct things: a high-level device architecture,
a parallel programming model for architectures with that design, or a software
platform that extends high-level languages like C to add that programming model.

The vision for CUDA is laid out in the
[Lindholm et al., 2008](https://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/lindholm08_tesla.pdf)
white paper. We highly recommend this paper, which is the original source for
many claims, diagrams, and even specific turns of phrase in NVIDIA's
documentation.

Here, we focus on the CUDA _software platform_.

The CUDA software platform is a collection of software for developing CUDA
programs. Though CUDA software platforms exist for other languages, like
FORTRAN, we will focus on the dominant
[CUDA C++](/gpu-glossary/host-software/cuda-c) version.

This platform can be roughly divided into the components used to _build_
applications, like the
[NVIDIA CUDA Compiler Driver](/gpu-glossary/host-software/nvcc) toolchain, and
the components used _within_ or _from_ applications, like the
[CUDA Driver API](/gpu-glossary/host-software/cuda-driver-api) and the
[CUDA Runtime API](/gpu-glossary/host-software/cuda-runtime-api), diagrammed
below.

![The CUDA Toolkit. Adapted from the *Professional CUDA C Programming Guide*.](themed-image://cuda-toolkit.svg)



================================================
FILE: gpu-glossary/host-software/cupti.md
================================================
---
title: What is the NVIDIA CUDA Profiling Tools Interface?
abbreviation: CUPTI
---

The NVIDIA CUDA Profiling Tools Interface (CUPTI) provides a set of APIs for
profiling execution of [CUDA C++](/gpu-glossary/host-software/cuda-c),
[PTX](/gpu-glossary/device-software/parallel-thread-execution), and
[SASS](/gpu-glossary/device-software/streaming-assembler) code on GPUs.
Critically, it synchronizes timestamps across the CPU host and the GPU device.

CUPTI's interfaces are consumed by, for example, the NSight Profiler and the
[PyTorch Profiler](/docs/examples/torch_profiling).

You can find its documentation [here](https://docs.nvidia.com/cupti/).

For details on using profiling tools for GPU applications running on Modal, see
[this example from our documentation](/docs/examples/torch_profiling).



================================================
FILE: gpu-glossary/host-software/libcuda.md
================================================
---
title: What is libcuda.so?
---

The typical name for the binary shared object file that implements the
[CUDA Driver API](/gpu-glossary/host-software/cuda-driver-api) on Linux systems.
It is dynamically linked by CUDA programs. If it is missing, the drivers are
generally improperly installed.



================================================
FILE: gpu-glossary/host-software/libcudart.md
================================================
---
title: What is libcudart.so?
---

The typical name for the binary shared object file that implements the
[CUDA Runtime API](/gpu-glossary/host-software/cuda-runtime-api) on Linux
systems. Deployed CUDA binaries often statically link this file, but libraries
and frameworks built on the CUDA Toolkit, like PyTorch, typically load it
dynamically.



================================================
FILE: gpu-glossary/host-software/libnvml.md
================================================
---
title: What is libnvml.so?
---

The typical name for the binary shared object file that implements the features
of [NVML](/gpu-glossary/host-software/nvml) on Linux systems.



================================================
FILE: gpu-glossary/host-software/nsight-systems.md
================================================
---
title: What is NVIDIA Nsight Systems?
---

NVIDIA Nsight Systems is a performance debugging tool for
[CUDA C++](/gpu-glossary/host-software/cuda-c) programs. It combines profiling,
tracing, and expert systems analysis in a GUI.

No one wakes up and says "today I want to write a program that runs on a hard to
use, expensive piece of hardware using a proprietary software stack". Instead,
GPUs are selected when normal computing hardware doesn't perform well enough to
solve a computing problem. So almost all GPU programs are performance-sensitive,
and the performance debugging workflows supported by Nsight Systems or other
tools built on top of the
[CUDA Profiling Tools Interface](/gpu-glossary/host-software/cupti) are
mission-critical.

You can find its documentation
[here](https://docs.nvidia.com/nsight-systems/index.html), but
[watching someone use the tool](https://www.youtube.com/watch?v=dUDGO66IadU) is
usually more helpful. For details on how to profile GPU applications on Modal,
see [our documentation](https://modal.com/docs/examples/torch_profiling).



================================================
FILE: gpu-glossary/host-software/nvcc.md
================================================
---
title: What is the NVIDIA CUDA Compiler Driver?
abbreviation: nvcc
---

The NVIDIA CUDA Compiler Driver is a toolchain for compiling
[CUDA C](/gpu-glossary/host-software/cuda-c)/C++ programs. It outputs binary
executables that conform to the host ABI and include
[PTX](/gpu-glossary/device-software/parallel-thread-execution) and/or
[SASS](/gpu-glossary/device-software/streaming-assembler) to be executed on the
GPU — a so-called "fat binary". These binaries are inspectable with the same
tools used for other binaries, like `readelf` on Linux, but can be additionally
manipulated with the specialized
[CUDA Binary Utilities](/gpu-glossary/host-software/cuda-binary-utilities).

The included [PTX](/gpu-glossary/device-software/parallel-thread-execution) code
is versioned by
[Compute Capability](/gpu-glossary/device-software/compute-capability),
configured by passing `compute_XYz` values to the `--gpu-architecture` or
`--gpu-code` options.

The included [SASS](/gpu-glossary/device-software/streaming-assembler) code is
versioned by
[SM architecture version](/gpu-glossary/device-hardware/streaming-multiprocessor-architecture),
configured by passing `sm_XYz` values to the `--gpu-architecture` or
`--gpu-code` options. Passing `compute_XYz` to `--gpu-code` will also trigger
the generation of [SASS](/gpu-glossary/device-software/streaming-assembler) code
with the same version as the
[PTX](/gpu-glossary/device-software/parallel-thread-execution).

Compilation of host/CPU code is done using the host system's compiler driver,
e.g. the `gcc` compiler driver. Note that compiler drivers are not to be
confused with hardware drivers, like the
[NVIDIA GPU Drivers](/gpu-glossary/host-software/nvidia-gpu-drivers).

The documentation for `nvcc` can be found
[here](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/).



================================================
FILE: gpu-glossary/host-software/nvidia-gpu-drivers.md
================================================
---
title: What are the NVIDIA GPU Drivers?
---

The NVIDIA GPU drivers mediate the interaction between host programs or the host
operating system and the GPU device. The primary interfaces to the GPU drivers
for applications are, in order, the
[CUDA Runtime API](/gpu-glossary/host-software/cuda-runtime-api) and the
[CUDA Driver API](/gpu-glossary/host-software/cuda-driver-api).

![The CUDA Toolkit. The NVIDIA GPU Driver, is the only component that communicates directly with the GPU. Adapted from the *Professional CUDA C Programming Guide*.](themed-image://cuda-toolkit.svg)

NVIDIA has released the
[source](https://github.com/NVIDIA/open-gpu-kernel-modules) for their Linux Open
GPU [Kernel Module](/gpu-glossary/host-software/nvidia-ko).



================================================
FILE: gpu-glossary/host-software/nvidia-ko.md
================================================
---
title: What is nvidia.ko?
---

`nvidia.ko` is a binary
[kernel module](https://wiki.archlinux.org/title/Kernel_module) file at the core
of the [NVIDIA GPU drivers](/gpu-glossary/host-software/nvidia-gpu-drivers) for
Linux.

Like other kernel modules, it executes in privileged mode and communicates
directly with hardware on behalf of the user -- in this case, the GPU.

The Linux Open GPU Kernel Module is
[open source](https://github.com/NVIDIA/open-gpu-kernel-modules).



================================================
FILE: gpu-glossary/host-software/nvidia-smi.md
================================================
---
title: What is nvidia-smi?
---

This command line utility is used to query and manage the state of the GPU
exposed by the [NVML](/gpu-glossary/host-software/nvml) management libraries.
Its outputs, a sample of which appears below, are familiar to users of NVIDIA
GPUs to the point of being a
[meme](https://x.com/boborado/status/1752724223934578760).

`nvidia-smi` reports the following:

- GPU identity information like the card's model name, a UUID, and the PCI ID
- live utilization metrics for kernel execution time and memory allocation
- live power and thermal information

For details on these metrics, including how to interpret power and thermal
readings, see [this page on the Modal docs](/docs/guide/gpu-metrics).

`nvidia-smi` can also list processes currently using the GPU (`-q`, `--query`,
`pmon`). Common management tasks include setting persistence mode (`-pm`),
compute mode (`-c`), power limits (`-pl`), application/locked clocks (`-ac`,
`-lgc`, `-lmc`), and performing GPU resets (`-r`).

Output can be formatted as human-readable text or XML (`-x`). While
`nvidia-smi`'s text output format is not guaranteed to be stable, the underlying
[NVML C library](/gpu-glossary/host-software/nvml) offers a stable API for tool
development.

The documentation for `nvidia-smi` can be found
[here](https://docs.nvidia.com/deploy/nvidia-smi/), and the official Python
bindings can be found [here](http://pypi.python.org/pypi/nvidia-ml-py/).

```
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:53:00.0 Off |                    0 |
| N/A   25C    P0             92W /  700W |       1MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   27C    P0             93W /  700W |       1MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:75:00.0 Off |                    0 |
| N/A   26C    P0             96W /  700W |       1MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:86:00.0 Off |                    0 |
| N/A   27C    P0             93W /  700W |       1MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:97:00.0 Off |                    0 |
| N/A   27C    P0             95W /  700W |       1MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:A8:00.0 Off |                    0 |
| N/A   25C    P0             91W /  700W |       1MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:B9:00.0 Off |                    0 |
| N/A   26C    P0             91W /  700W |       1MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:CA:00.0 Off |                    0 |
| N/A   24C    P0             91W /  700W |       1MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
```



================================================
FILE: gpu-glossary/host-software/nvml.md
================================================
---
title: What is the NVIDIA Management Library?
abbreviation: NVML
---

The NVIDIA Management Library (NVML) is used for monitoring and managing the
state of NVIDIA GPUs. It exposes, for example, the power draw and temperature of
the GPU, the allocated memory, and the device's power limit and power limiting
state.

The function of NVML are frequently accessed via the
[nvidia-smi](/gpu-glossary/host-software/nvidia-smi) command line utility, but
are also accessible to programs via wrappers, like
[pynvml in Python](https://pypi.org/project/pynvml/) and
[nvml_wrapper in Rust](https://docs.rs/nvml-wrapper/latest/nvml_wrapper/).



================================================
FILE: gpu-glossary/host-software/nvrtc.md
================================================
---
title: What is the NVIDIA Runtime Compiler?
abbreviation: nvrtc
---

The NVIDIA Runtime Compiler (`nvrtc`) is a runtime compilation library for CUDA
C. It compiles [CUDA C++](/gpu-glossary/host-software/cuda-c) to
[PTX](/gpu-glossary/device-software/parallel-thread-execution) without requiring
a separate launch of the
[NVIDIA CUDA Compiler Driver](/gpu-glossary/host-software/nvcc) (`nvcc`) in
another process. It is used by some libraries or frameworks to, for example, map
generated C/C++ code to
[PTX](/gpu-glossary/device-software/parallel-thread-execution) code that can run
on a GPU.

Note that this [PTX](/gpu-glossary/device-software/parallel-thread-execution) is
then further JIT-compiled from the
[PTX](/gpu-glossary/device-software/parallel-thread-execution) IR to the
[SASS assembly](/gpu-glossary/device-software/streaming-assembler). This is done
by the [NVIDIA GPU drivers](/gpu-glossary/host-software/nvidia-gpu-drivers) and
is distinct from the compilation done by NVRTC. CUDA binaries that contain
[PTX](/gpu-glossary/device-software/parallel-thread-execution), as required for
forward compatibility, also pass through this compilation step.

NVRTC is closed source. You can find its documentation
[here](https://docs.nvidia.com/cuda/nvrtc/index.html).


